% -----------------------------------------------------------------------------%
% Title:
% % Author: Jan Ian Failenschmid % Created Date: 13-03-2024
% %
% -----                                                                        %
% Last Modified: 11-09-2024                                                    %
% % Modified By: Jan Ian Failenschmid
% %
% %
% %
% -----                                                                        %
% Copyright (c) 2024 by Jan Ian Failenschmid
% % E-mail: J.I.Failenschmid@tilburguniveristy.edu
% %
% -----                                                                        %
% License: GNU General Public License v3.0 or later
% % License URL: https://www.gnu.org/licenses/gpl-3.0-standalone.html
% %
% -----------------------------------------------------------------------------%

\documentclass[man, floatsintext]{apa7}

% Dependencies
\usepackage{csquotes, amsmath, amssymb, mathptmx, enumitem, array}
\usepackage[american]{babel}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa} \graphicspath{ {./figures} }
% Gaphics Path
\addbibresource{bibliography.bib} % Literature bibliography
\addbibresource{R_bib.bib} % R bibliography
\DeclareMathOperator*{\argmin}{argmin}
\setcounter{secnumdepth}{3}
\parindent=0pt
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% Title Page
\title{Modeling Non-Linear Psychological Processes: Reviewing and Evaluating
  (Non-) parametric Approaches and Their Applicability to Intensive
  Longitudinal Data}

\shorttitle{Modelling Non-Linear Psychological Processes}

\leftheader{Failenschmid}

\authorsnames{{Jan I. Failenschmid}, {Leonie V.D.E Vogelsmeier}, {Joris
      Mulder}, {Joran Jongerling}}

\authorsaffiliations{{Tilburg University}}

\abstract{Here could your abstract be!}

\keywords{Here could your keywords be!}

\authornote{ \addORCIDlink{Jan I. Failenschmid}{0009-0007-5106-7263}

  Correspondence concerning this article should be addressed to Jan I.
  Failenschmid, Tilburg School of Social and Behavioral Sciences: Department of
  Methodology and Statistics, Tilburg University, Warandelaan 2, 5037AB
  Tilburg, Netherlands. E-mail: J.I.Failenschmid@tilburguniversity.edu}

\begin{document}

\maketitle

Psychological constructs are increasingly understood as components of complex
dynamic systems \parencite{nesselroade_studying_2004, wang_investigating_2012}.
This perspective emphasizes that these constructs fluctuate over time and
within individuals. To study these variations and the underlying processes,
researchers are increasingly collecting intensive longitudinal data (ILD) using
ecological momentary assessment (EMA), experience sampling, or similar methods
\parencite{fritz_so_2023}. In these studies one or more individuals are
assessed at a high frequency (multiple times per day) using brief
questionnaires or passive measurement devices. These rich data allow
researchers to examine complex temporal variations in the underlying
psychological variables within an ecologically valid context and to explain
them through (between-person differences) in within-person processes.

Due to these ILD studies, many non-linear psychological phenomena and processes
have been discovered during recent years. Clear examples of this are the
learning and growth curves observed in intellectual and cognitive development
\parencite{kunnen_dynamic_2012, mcardle_comparative_2002}. In these cases, an
individual's latent ability increases over time, following an intricate
non-linear trajectory from a (person specific) starting point towards an
(person specific) asymptote, which reflects the inidividual's maximum ability.
Additional examples of asymptotic growth over the shorter time spans that are
typically studied with ILD include motor skill development
\parencite{newell_time_2001} and second language acquisition
\parencite{de_bot_dynamic_2007}. Figure~\ref{fig:examplar_npn} shows
common model choices for these kinds of processes in the form of an
exponential growth function (a) and a logistic growth function (b).

\begin{figure}[!t]
  \caption{Examples of non-linear processes demonstrated to occur in
    psychological time series}
  \fitfigure{exemplar_no_process_noise.png}
  \figurenote{This figure shows four demonstrated psychological non-linear
    processes. Panels (a) and (b) show exponential and logistic growth curves,
    respectively. Panel (c) shows a cusp catastrophe model. Lastly, panel (d)
    shows a damped oscillator.}
  \label{fig:examplar_npn}
\end{figure}

Another common non-linear phenomenon is that the construct of interest switches
between distinct states, which often correspond to different mean levels. This
occurs, for example, during the sudden perception of cognitive flow, where
individuals abruptly switch from a `normal' state to a flow state and back
\parencite{ceja_suddenly_2012}. Another example is alcohol use relapse, where
patients suddenly switch from an abstinent state to a relapsed state
\parencite{witkiewitz_modeling_2007}. This sudden switching behavior has been
modelled using a cusp catastrophe model. This dynamic model, drawn from
catastrophe theory, naturally leads to mean level switches when varying one of
its parameters \parencite{van_der_maas_sudden_2003,chow_cusp_2015} and has been
exemplified in Figure~\ref{fig:examplar_npn}~(c).

As a final example, one may consider (self-) regulatory systems, which maintain
a desired state by counteracting external perturbations. In these systems the
regulatory force often depends on the distance
between the current and the desired states. The common autoregressive model
describes such a system in which the regulation strength depends linearly on
this distance. However, this relationship may also be non-linear, such that the
regulatory force changes disproportionately with larger mismatches. Such a
(self-) regulatory model has been used to model, for example, emotion
regulation \parencite{chow_emotion_2005} using a damped oscillator model. This
model is exemplified in Figure~\ref{fig:examplar_npn}~(d).

Although initial evidence for non-linearity in psychological research exists,
theories about the nature and form of non-linear psychological processes remain
scarce \parencite{tan_time-varying_2011}. Frequently, psychological theories
are too general \parencite{oberauer_addressing_2019} to predict specific
non-linear dynamics and ILD studies could have the
potential to refine these theories through a nuanced understanding of how the
involved psychological variables interact over time. Such refined
theories could, for instance, take the form of formal dynamic models, such as
differential equation \parencite{cooper_dynamical_2012} or state-space models
\parencite{durbin_time_2012}, describing how a given process changes over time.
However, in order to develop these types of theories it is first necessary to
identify phenomena in ILD, which are replicable and empirically observable
features of the underlying processes, such as the described state switching or
regulatory oscillations. Formal theories about the underlying process should
then be able to explain these phenomena and different candidate theories can be
compared on their success to do so \parencite{borsboom_theory_2021}. While the
study of non-linear phenomena in ILD is receiving increasingly more attention
in psychology and different statistical techniques are developed to explore
these phenomena \parencite{cui_unlocking_2023,humberg_estimating_2024},
researchers are currently still limited in their ability to infer non-linear
phenomena from ILD due to a lack of advanced statistical methods that are
flexible enough to adequately capture and explore these processes, which
hinders the development and evaluation of guiding theories. Due to this lack of
adequate available statistical methods, non-linear trends are most often
addressed in psychology through polynomial regression or
piecewise spline regression.

Polynomial regression \parencite{jebb_time_2015} uses higher-order terms (e.g.,
squared or cubed time) as predictors in a standard multiple linear regression
model. While effective for relatively simple non-linear relationships,
particularly those that can be represented as polynomials, this method has
significant limitations and likely leads to invalid results when applied to
more complex processes, such as mean switching or (self-) regulatory
systems (e.g., Figure~\ref{fig:examplar_npn}~c~\&~d). In these cases,
polynomial approximations require many higher-order terms to capture the
process's complex trajectory, which raises the problem of over- or underfitting
the data, causes model instability, and leads to nonsensical inferences (e.g.,
interpolating scores outside the scale range;
\textcite{boyd_divergence_2009,harrell_general_2001}).

An alternative approach is piecewise spline regression, which constructs a
complex non-linear trend by joining multiple simple piecewise functions at
specific points, called knots (e.g., combining multiple cubic functions into a
growth curve with plateaus; \textcite{tsay_nonlinear_2019}). However, spline
regression requires a careful, manual selection of the optimal piecewise
functions and knot locations. This can be problematic in practice because, as
mentioned, precise guiding theories about the functional form of most
psychological processes are lacking \parencite{tan_time-varying_2011}. This
absence of clear guidance can quickly lead to misspecified models and invalid
results.

These limitations in the currently available methods underscore the need for
more sophisticated statistical methods to study and explore non-linear
processes. Various such advanced statistical methods, such as kernel
regression, Gaussian processes, and smoothing splines are available outside of
psychology. However, these methods have rarely been applied in psychology
because they have not been reviewed for an applied audience, nor have their
assumptions and inference possibilities been evaluated in the context of ILD.\@
As a result, psychological researchers struggle to select the most suitable
method for a specific context. This challenge is further complicated by the
fact that the ideal statistical method may depend on the characteristics of the
underlying non-linear process, which are generally unknown. Especially, since
the assumed smooth processes for which many of these methods were originally
developed are unlikely to occur in psychological research.

To address this important gap, this article reviews three advanced non-linear
analysis methods and evaluates their applicability to typical ILD scenarios
(Section~\ref{method_introduction}). The methods reviewed in this article are
different semi- and non-parametric regression techniques that are able infer
non-linear functions from data while accommodating varying degrees of prior
knowledge. We compare how well each method can recover different processes
under common ILD conditions in a simulation study (Section~\ref{simulation}).
Lastly, we also demonstrate how some of the introduced methods can by applyed
to analyze an existing dataset (Section~\ref{empirical_example}). Further, to
introduce these methods accessibly and apply them under conditions where
software implementations are available, this article focuses on the univariate
single-subject design.

\section{Non-linear analysis methods}\label{method_introduction}

\subsection{Local polynomial regression}

The first technique is called local polynomial regression (LPR). Similarly to
regular polynomial regression, LPR approximates the process using polynomial
basis functions (e.g., squared or cubed time). However, instead of using one
large polynomial function to approximate the entire process, LPR estimates
smaller, local polynomials at any point in time. These local polynomials are
then combined into a single non-linear function over the entire set of
observations \parencite{fan_adaptive_1995, ruppert_multivariate_1994,
  fan_local_2018}.

To determine the process value that the LPR predicts at a specific time point,
the data is first centered around that point (by shifting the data along the
time axis so that the chosen time point is at zero), and a low-order polynomial
is fitted around it. Additionally, to account for the fact that the polynomial
approximation is more accurate for data points closer in time, a weighting
function is applied during the polynomial estimation, which assigns weights to
each data point based on its distance from the point of interest. The value
that the LPR predicts for the chosen time point is then given by the intercept
of the locally weighted polynomial at this point in time.

Formally this procedure can be expressed using the following set of equations:

\begin{equation} \label{eq:lpr_equations}
  \begin{aligned}
    y_t          & = f(t) + \epsilon_t                            \\
    \textbf{X}   & =
    \begin{bmatrix}
      1      & (t_1 - t^*)^1 & \dots  & (t_1 - t^*)^p \\
      \vdots & \vdots        & \ddots & \vdots        \\
      1      & (t_n - t^*)^1 & \dots  & (t_n - t^*)^p
    \end{bmatrix} \\
    \textbf{W}   & =
    \begin{bmatrix}
      w_{1, 1} &        &          \\
               & \ddots &          \\
               &        & w_{n, n}
    \end{bmatrix}                               \\
    \hat{f}(t^*) & =
    Intercept((\textbf{X}^T\textbf{WX})^{-1}\textbf{X}^T\textbf{Wy})
  \end{aligned}
\end{equation}

\noindent where a univariate process $f$ is inferred at the
chosen time point $t^*$. Then \textbf{X} is the model matrix of a multiple
linear regression, such that the columns correspond to polynomial
transformations up to degree $p$ of the data centered around
$t^*$\footnote{Note
  that the polynomial terms in this model matrix are derived from a Taylor
  series approximation around $t^*$ with $p$ derivative terms.}. Further,
\textbf{W} is a diagonal matrix containing the weights associated with each
datum. The last equation is a normal equation solving for the coefficients of a
weighted multiple linear regression. The intercept of this regression
gives the estimated value of the LPR at $t^*$.

To find the process value that the LPR predicts for a different time point, the
same procedure can be repeated, centering the data around the new point of
interest. Since it is theoretically possible to repeat this process at
infinitely many time points, LPR is a non-parametric technique.
Figure~\ref{fig:locpol_dem} shows the estimated LPR for the damped oscillator
example process introduced in Figure~\ref{fig:examplar_npn}~(d). In this
figure, three truncated examples of local cubic regressions are shown in red,
which contribute to the overarching LPR for this process.\@

\begin{figure}[!t]
  \caption{Demonstration of a local polynomial regression}
  \fitfigure{locpol_demonstration.png}
  \figurenote{This figure shows how LPR (solid black) estimates the underlying
    process (dotted black). Here, three exemplary local cubic regressions (red)
    are shown that provide the values that the LPR predicts at the time points
    50, 100, and 150. The complete LPR was plotted by evaluating
    such local cubic regressions at 200 points along the time axis.}
  \label{fig:locpol_dem}
\end{figure}

When fitting an LPR, several decisions must be made regarding the degree of the
local polynomials and the optimal weighting of the data. Typically, the degree
of these local polynomials is kept low and odd. This choice reflects a
bias-variance tradeoff, where higher-order polynomials reduce bias but increase
variance only when transitioning from an odd to an even power
\parencite{ruppert_multivariate_1994}. The data weighting in an LPR is achieved
through a kernel function of the form $w_{i, i} = K(\frac{t_i - t^*}{h})$,
which is usually centered and symmetric to assign weights based on the distance
from the origin. Common kernel choices include the Gaussian and Epanechnikov,
with the latter being optimal in many applications and aspects
\parencite{fan_local_1997}. Each kernel is further defined by a bandwidth
parameter $h$, which determines its width and effectively controls the
influence of more distant data points. The bandwidth parameter represents the
wiggliness of the estimated process in practice. Several methods are available
to find the optimal bandwidth by optimizing a data-dependent criterion
function, such as the cross-validation error or the mean integrated squared
error \parencite{kohler_review_2014, debruyne_model_2008}.

Due to its non-parametric nature, LPR makes minimal assumptions about the data.
However, it does require that the underlying process is $p$ times
differentiable, which is a necessary condition for local polynomial
approximation (under Taylor's theorem). It is further noteworthy
that unless the process follows a polynomial of at most degree $p$ the
approximation with local polynomials is biased. However, this bias is usually
negligible and there are methods available to correct for it
\parencite{R-nprobust}. Another key assumption is that the process has constant
wiggliness, represented by a single bandwidth parameter. However, this
assumption may be relaxed by using a time-varying bandwidth
\parencite{fan_data-driven_1995} or polynomial degree
\parencite{fan_adaptive_1995}, but this extension is beyond the scope of this
paper.

\subsection{Gaussian process regression}

The second non-parametric technique is Gaussian process (GP) regression, a
Bayesian approach that directly defines a probability distribution over an
entire family of non-linear functions, which is flexible enough to capture many
complex processes effectively \parencite{rasmussen_gaussian_2006,
  betancourt_robust_2020, roberts_gaussian_2013}. Unlike regular probability
distributions (e.g., normal distribution) that specify the plausibility of
single values, Gaussian processes determine the plausibility of entire
(non-linear)  functions. A GP is defined such that the functions it describes,
take values at any finite set of time points that follow a multivariate normal
distribution. In a Bayesian framework, one can use a GP to define a prior
distribution for the latent process, as $P(f) \sim GP$. This prior is then
combined with an appropriate likelihood for the observed data to obtain a
posterior distribution for the latent process given the observed data.

\begin{equation}
  P(f \, | \, \textbf{y})  \propto P(\textbf{y} \, | \, f) P(f)
\end{equation}

\noindent This posterior distribution represents an updated belief about which
functions describe the latent process well \parencite{kruschke_doing_2011},
allowing one to draw inferences about the process itself.
Figure~\ref{fig:gp_dem} illustrates such a posterior distribution for the
running example process. The red lines represent a sample of non-linear
functions drawn from the posterior distribution, such that the pointwise
average of these functions provides a mean estimate for the underlying process.

\begin{figure}[!t]
  \caption{Demonstration of a Gaussian process regression}
  \fitfigure{gp_demonstration.png}
  \figurenote{This figure shows how a Gaussian process regression estimates the
    underlying process (dotted black). Here, a sample of functions drawn from
    the posterior Gaussian process probability distribution with a squared
    exponential kernel is shown (red). The predicted value for the underlying
    process is then obtained by averaging the drawn functions.}
  \label{fig:gp_dem}
\end{figure}

The GP prior is parameterized by a mean function $m(t)$ and a covariance
function $cov(t, \, t)$, which are continuous extensions of the mean vector and
covariance matrix of a multivariate normal distribution. These functions can be
selected based on domain knowledge or through data-driven model selection
\parencite{richardson_gaussian_2017, abdessalem_automatic_2017}. In practice,
the mean function is often set to zero when no specific prior knowledge is
available. This does not constrain the posterior mean to zero but instead
indicates a lack of prior information about its deviations from zero. The
covariance function is typically based on a kernel function, which assigns
covariances between time points depending on their distance (e.g.,
quadratic exponential, Matern class kernels), such that

\begin{equation}
  cov(t_i, t_j) = k(|t_i - t_j|)
\end{equation}

For some combinations of GP piors and likelihoods, is possible to derive an
analytic posterior distribution for a GP\@. One example of this is a GP prior
with a zero mean function and a Gaussian likelihood with a zero mean and a
standard deviation of $\sigma$. Then the posterior process values at a selected
set of time points $\textbf{t}^*$ are distributed according to a multivariate
normal distribution with a mean vector and covariance matrix of:

\begin{equation}
  \begin{aligned}
    f(\textbf{t}^* \, | \, \textbf{y}) & \sim MvN(\mu(\textbf{t}^*),
    \Sigma(\textbf{t}^*))
    \\
    \mu(\textbf{t}^*)                  & = K(\textbf{t}^*,
    \textbf{t}){[K(\textbf{t},
            \textbf{t}) + \sigma^2
            \textbf{I}]}^{-1}\textbf{y}
    \\
    \Sigma(\textbf{t}^*)               & =K(\textbf{t}^*, \,
    \textbf{t}){[K(\textbf{t},
            \textbf{t}) + \sigma^2
            \textbf{I}]}^{-1} K(\textbf{t}, \textbf{t}^*)
  \end{aligned}
\end{equation}

\noindent where $\textbf{K}$ is a matrix collecting the covariances given by
$cov(t, t)$ at the observation time points $\textbf{t}$ and the evaluation time
points $\textbf{t}^*$ (which may also be the same).

Laslty, the the mean and covariance function typically contain parameters
themselves, which can be treated as hyperparameters and can be estimated as
such by specifying appropriate priors. These hyperpriors reflect prior beliefs
about the hyperparameters and can be used to constrain them to sensible values.
The corresponding posterior distributions then provide interpretable inferences
for the hyperparameters. Most common covariance kernels build on a
characteristic lengthscale and a marginal standard deviation parameter. The
characteristic lengthscale effectively determines the wigglyness of the
estimated process, by qunatifying how quickly the covariance decreases with
increasing distances between time points. The marginal standard deviation
describes the spread of the functions which are described by the GP at any
point in time. However, GP regression can also include additional
hyperparameters, allowing for more specific theories to be tested through model
comparison.

Whereas LPR is a mainly data driven procedure GP regression is more model
based. The GP prior generates a family of functions to which the process is
assumed to belong. This makes it possible to include theoretical domain
knowledge and specific hypotheses of interest in the kinds of function that are
modelled by a GP\@. One example of this could be to combine in one model a
linear mean function with non-linear deviations captured by a GP\@. However,
this also means that to accurately capture a process, it is important that the
functional familiy generated by a GP is similar to the actual process. Most
common choices for covariance kernels for example result in smooth and
covariance-stationary GPs with constant wigglyness. Another difference between
GP regression and LPR is that the Bayesian estimation underlying the GP
regression provides a natural approach to uncertainty quantification. While
there is no direct way of quantifying the uncertainty associated with the
bandwidth of the LPR, the uncertainty of the lengthscale of the GP is captured
in its posterior distribution.

\subsection{Generalized additive models}

Generalized additive models (GAM) are a class of semi-parametric models that
build on so called smooth terms, which are non-linear functions that are
inferred from the data through smoothing splines
\parencite{wood_generalized_2006, wood_inference_2020,
  hastie_generalized_1999}. Smoothing splines extend regular spline regression
to
mitigate the knot placement problem by providing enough flexibility to the
spline to guarantee overfitting the data. One approach to this, is to use cubic
splines, which are piecewise cubic polynomials. By placing a knot at each data
point these splines perfectly interpolate the data. Alternatively, thin-plate
splines can be used, which entirely avoid knots and instead utilize
increasingly wiggly basis functions that are defined over the entire range of
the data \parencite{wood_thin_2003}. These basis functions are then combined in
a regression model similar to polynomial regression. Here, using as many basis
functions as there are data points is analogous to placing a knot at each data
point. To prevent the resulting overfitting, smoothing splines add an
additional penalty term, similar to those used in a lasso or ridge regression,
to control the smooth term's flexibility during the estimation
\parencite{gu_smoothing_2013, wahba_spline_1980}. This penalty balances the
flexibility and fit of the smooth term, ensuring the model captures the
underlying process accurately without introducing unnecessary complexity. The
optimal weight of the penalty is typically determined by minimizing a criterion
function, such as the generalized cross-validation criterion or thorough
likelihood maximization \parencite{wood_generalized_2006,
  golub_generalized_1997}.

A smoothing spline for a single smooth term $\beta$ may then be written as

\begin{equation}
  \begin{aligned}
    \hat{\beta}(t) & = \argmin_\alpha \, P(\textbf{y} \, | \, \beta(t)) +
    \lambda \int {(\beta(t)'')}^2 dt                                      \\
    \beta(t)       & = \sum^K_{k = 1} \alpha_k R_k(t)
  \end{aligned}
\end{equation}

\noindent where the first part of the equation describes the likelihood
of the data given the smooth term and the second part of the equation
corresponds to the penalty term. This illustrates, how the smoothing
spline balances data fit, in the form of the likelihood, and the complexity or
wigglyness of the estimate, measured by the integrated squared second
derivative of the smooth term. Here, $\lambda$ denotes the weight assigned to
the penalty term that is optimized over. Lastly, the smoothing spline is
comprised of spline basis functions $R_k(t)$, such as the introduced cubic or
thin-plate splines and their respective regression coefficients $\alpha_k$.

GAMs extend on the smoothing spline approach by making it possible to combine
multiple smooth terms in an overarching additive regression model, where each
smooth term essentially functions as a predictor within a regular regression
analysis. In this model, smooth terms (with potentially different input
variables) may be multiplied by covariates $x_j$ and
summed into a single overall non-linear function $f$, which estimates the
process.

\begin{equation}
  \begin{aligned}
    \hat{f}(t) & = \argmin_\alpha \, P(\textbf{y} \, | \, f(t)) +
    \sum_{j = 1}^{J} \lambda_j \int {(\beta_j(t)'')}^2 dt         \\
    f(t)       & = \sum_{j = 1}^{J} \beta_j x_j                   \\
    \beta_j(t) & = \sum^K_{k = 1} \alpha_{j,k} R_{j, k}(t)
  \end{aligned}
\end{equation}

\noindent This approach makes it possible to formulate models such
as a time-varying autoregressive model, where the intercept and autoregressive
parameters are smooth terms of time \parencite{bringmann_changing_2017,
  bringmann_modeling_2015}. By integrating non-parametric smooth terms into a
broader parametric model, GAMs become semi-parametric models that are
well-suited for testing specific hypotheses while keeping the flexibility
needed to accurately capture the latent process. Figure~\ref{fig:gam_dem}
illustrates a simple GAM construction with a single smooth term for time,
fitted to the example process. The first ten thin-plate smoothing spline basis
functions of the nearly 200 basis functions that make up the smooth term are
shown in red.

\begin{figure}[!t]
  \caption{Demonstration of the construction of a GAM}
  \fitfigure{gam_demonstration.png}
  \figurenote{This figure shows how generalized additive models (solid black)
    estimate the underlying process (dotted black). Here, the predicted values
    for the process at any point in time correspond to the weighted average of
    the basis functions (red).}
  \label{fig:gam_dem}
\end{figure}

Similar to GPs, GAMs constitute a more model based approach to inferring
non-linearity. Indeed, GAMs and GP regression are closely related technique,
since each GAM can be understood as a (unconventional) GP with a partially
improper prior\footnote{Any basis function regression with Gaussian priors for
  the coefficients is a GP in weight space. The smoothing spline underlying the
  GAMs is such a basis function regression in which the coefficient priors
  serve
  the same purpose as the penalty term. Due to the null space of the penalty
  term, some of these priors are generally going to be imporper.}. These
improper
priors make the GAMs more flexible to model processes, which might be outside
the functional family generated by a standard GP construction
\parencite{wahba_improper_1978}. In addition to this and similar to GPs, GAMs
offer an intuitive way to combine partial theories with data driven smooth
terms. In this way, GAMs can also, for instance, model a linear function with
non-linear deviations, which are captured by a smooth term. GAMs also provide
an estimate of the wigglyness of the process through the weight of the smooth
term. In contrast to LPR and the standard GP construction, GAMs do not assume
constant wigglyness. Table~\ref{tab:meth_sum} summarizes the similarities and
differences between the three introduced methods.

\begin{table}[!t]
  \begin{center}
    \begin{threeparttable}
      \caption{A comparison of LPR, GP regression and GAMs}
      \label{tab:meth_sum}
      \begin{tabular}{
          L{\dimexpr 0.25\linewidth-2\tabcolsep}
          L{\dimexpr 0.25\linewidth-2\tabcolsep}
          L{\dimexpr 0.25\linewidth-2\tabcolsep}
          L{\dimexpr 0.25\linewidth-2\tabcolsep}}
        \toprule
                                                                       &
        \multicolumn{1}{c}{LPR}
                                                                       &
        \multicolumn{1}{c}{GP}                                         &
        \multicolumn{1}{c}{GAM}
        \\
        \midrule
        Unit of Analysis                                               &
                                                                       &
                                                                       &
        \\
        Data Structure                                                 &
                                                                       &
                                                                       &
        \\
        Extensions                                                     &
        Variable bandwidth and polynomial degrees                      &
        Many in theory, few implemented in software                    &
        Multivariate and multilevel input data
        \\
        Key assumptions                                                &
        Process should be p-times differentiable, constant wigglyness  &
        Process should be similar to the functions described by the GP &
        Process should be smooth, homoscedasticity
        \\
        Estimation                                                     &
        OLS                                                            &
        Bayesian                                                       &
        OLS, MLE, Bayesian
        \\
        Key sources of information                                     &
        \textcite{fan_local_2018}                                      &
        \textcite{rasmussen_gaussian_2006}                             &
        \textcite{wood_generalized_2006}
        \\
        \bottomrule
      \end{tabular}
    \end{threeparttable}
  \end{center}
\end{table}

\section{Simulation} \label{simulation}

\subsection{Problem}

A simulation study was conducted to assess the effectiveness of the introduced
methods in recovering different non-linear processes, which may be encountered
in EMA research (Figure~\ref{fig:examplar_npn}). In addition to that, the
introduced methods were also compared to a polynomial regression, which is
currently used most often to model non-linear trends in psychology. Lastly, to
obtain a benchmark for how accuractely non-linear processes can be recovered
when their parametric form is correctly specified, the data-generating
parametric models were also included in the simulation. To apply the
introduced methods under the conditions described in the introduction, and
within the constraints of available software implementations, the simulation
focused on a univariate single-subject design. Hence, the simulated data
represented repeated measurements of a single variable for one individual.

\subsection{Design and Hypotheses}

For the LPR and the GP, we expect that both method, using the configurations in
which they are most often applied and implemented in standard software, will
most accurately infer processes that are (a) continuous (i.e., without sudden
jumps), (b) have constant wigglyness, and (c) are smooth (i.e.,
differentiable). We expect this, because both methods by default produce
continuous, smooth estimates with a single constant bandwidth or lengthscale.
For the GAMs, we expect that only criteria (a) and (c) will influence the
performance, as GAMs do not assume constant wigglyness. The parametric modeling
approach is expected to provide the most accurate inferences, serving as a
benchmark for comparison with the other methods. Additionally, we expect that
the overall sample size will influence the accuracy of the inferences, with
both (d) the overall length of the sampling period and (e) the sampling
frequency being varied.

To conduct the simulation with processes that might be encountered in real EMA
studies, we selected the exemplar processes illustrated in
Figure~\ref{fig:examplar_npn} as a basis. These include two growth curves,
modeled as an exponential and a logistic growth curve, a mean-level switching
process, modeled as a cusp catastrophe, and a self-regulatory process,
represented by a damped oscillator. These processes make it possible to test
the impact of (a) sudden jumps and (b) changing wigglyness on the four methods.
First, we hypothesize that the cusp catastrophe model, which is the only
process featuring jumps, will be least accurately inferred by all methods.
Second, all four processes exhibit changes in wigglyness (i.e., changes in the
second derivative) over time. However, while the wiggliness of the exponential
and logistic growth functions and the damped oscillator decreases
monotonically, the cusp catastrophe's wigglyness changes cyclically (i.e., low
wigglyness during the plateau phases and very high wigglyness during the
jumps). Therefore, we hypothesize that longer sampling periods for the
exponential and logistic growth curves and the damped oscillator will reduce
the inference accuracy of the LPR and the GP, as the single bandwidth or
lengthscale parameter becomes increasingly inadequate to capture the changing
wigglyness over time. We do not expect this effect to occur for the cusp
catastrophe process, or when using GAMs.

To manipulate the (c) smoothness of the processes a dynamic error component was
added to each process. These dynamic errors reflect external perturbations to
the latent construct that are carried forward in time. For instance, if a
participant experiences an unusually pleasant conversation that elevates their
true positive affect, this change represents an error effect if it is not
accounted for by the model. However, since the true positive affect level has
increased, this will influence future measurements due to for example emotional
inertia. To add these errors, each process was perturbed by a normally
distributed error at each point in time, resulting in non-smooth (i.e.,
non-differentiable or rough) trajectories. The degree of roughness was
controlled by the variance of these dynamic errors and we considered variances
of 0.5, 1, and 2 reasonable relative to the process range.
Figure~\ref{fig:exemplar_pn} illustrates one possible realization of the
exemplar processes with dynamic errors. Importantly, we intentionally omitted a
condition without dynamic noise from this simulation, as dynamic noise is
reasonably expected to be present in all psychological intensive longitudinal
data (ILD).

\begin{figure}[!t]
  \caption{Non-linear exemplar processes with dynamic errors}
  \fitfigure{exemplar_process_noise.png}
  \figurenote{This figure shows one possible realization of how the exemplar
    processes could unfold with dynamic errors. The processes have dynamic
    error standard deviations of 0.25 (left) and 0.5 (right).}
  \label{fig:exemplar_pn}
\end{figure}

Additionally, the sample size was varied during the simulation by manipulating
both (d) the sampling period and (e) the sampling frequency, as these distinct
methodological choices are expected to impact the performance of the analysis
methods differently. Specifically, for the LPR and the GP, which rely only on
data in local neighborhoods during the estimation, we expected that extending
the sampling period beyond this neighborhood will not increase the inference
accuracy. In fact, if the process exhibits changing wigglyness over the
extended period, as previously discussed, increasing the sampling period might
even negatively affect the inference accuracy. In contrast to this, we expected
the GAMs, which incorporate the entire dataset in their estimation, to perform
better with a longer sampling period. Since within the simulation there is no
inherent scaling to the time variable, we simulated either only the first half
of each process or the entire process to represent different sampling periods.
For the ease of reading and to correspond to typical EMA conditions, this will
be referred to as sampling over either one or two weeks. However, this scaling
is arbitrary and could be changed to any other time frame. Lastly, we expected
that increasing the sampling frequency will generally improve the inference
accuracy across all methods, since there is more information about the latent
process available. Relative to the introduced weekly scale, we tested sampling
frequencies of three, six, and nine measurements per day, to cover typical EMA
sample sizes \parencite{wrzus_ecological_2023}.

\subsection{Data generation}

To simulate data for each process, they must first be represented as parametric
generative models that replicate the structure of psychological time series
data. In such a time series, any psychological construct follows a (potentially
non-linear) function over time, as depicted by the lines in
Figure~\ref{fig:examplar_npn}. However, since these psychological processes are
typically unobservable or latent, they are measured through observable
indicators, such as questionnaire items. The observed values on these
indicators (Figure~\ref{fig:examplar_npn}, dots) differ from the true values of
the latent process due to measurement error, which may come from an imperfect
measurement instrument. For this simulation, we assume that all
time-point-specific measurement errors are independent and normally
distributed. The model for the observations of a single indicator can then be
expressed as follows:

\begin{align}
  Y_t = f(t) + \epsilon_t; \quad \epsilon_t \sim N(0, \sigma^2_{\epsilon})
\end{align}

\noindent In this model, $f(t)$ represents a potentially non-linear latent
process (like the ones presented in Figure~\ref{fig:examplar_npn}, although
other processes are also possible), and $\epsilon_t$ represents the
time-point-specific measurement error. The most direct way to define a
parametric model for $f(t)$ is as a (non-linear) function of time.
Unfortunately, many processes have functional forms that are too complex for
this representation, and this approach does not allow for the modeling of
dynamic errors.

Instead, each process was represented as a generative stochastic differential
equation model. These dynamic models describe the relationship between the
process's current value and its instantaneous rate of change. This can be
combined with information about the initial state of the process to infer the
entire trajectory indirectly. Differential equations are a widely used class of
dynamic models because they can capture complex processes in much simpler
parametric forms. Nevertheless, they do require considerable theoretical
knowledge about the process for applied modelling. For instance, a differential
equation model representing the introduced logistic growth process can be
expressed as follows:

\begin{equation} \label{eq:2}
  \frac{dy}{dt} = r y (1-\frac{y}{k})
\end{equation}

\noindent This model relates the rate of change of $y$ to its current value and
to how far away the current value is from the asymptote $k$ through a growth
rate constant $r$.

There are different ways to add dynamic errors to differential equation models.
However, the most common modelling choice is to add an additive Wiener process
to the deterministic model.

\begin{equation}
  dy = r y (1-\frac{y}{k})dt + \sigma dW_t
\end{equation}

\noindent The Wiener process is a continuous non-differentiable stochastic
process, which describes normally distributed dynamic errors over any given
discrete time interval. These errors have a mean of zero and a variance
dependent on the length of the time interval and $\sigma$, making them optimal
for this simulation. Importantly, these dynamic errors continuously influence
the rate of change of the process and are propagated forward in time through
the deterministic dynamics of the model.

All four processes were modeled as stochastic differential equations by
substituting their respective deterministic dynamics into Equation~\ref{eq:2},
as detailed in Appendix B. Latent process data were simulated using the
Euler-Maruyama method, which approximates stochastic differential equation
systems with an arbitrarily high accuracy by linearizing them over small
discrete time intervals. The resulting high-resolution data were then
subsampled to achieve the desired sampling frequency. Finally, measurement
errors were added to the latent process data at each time point from a standard
normal distribution, generating the final sets of observations

To determine the required number of data sets per condition, a power simulation
was conducted based on an initial pilot sample of 30 generated data sets per
condition. Based on the pilot sample, the outcome measures (e.g., MSE, GCV, and
confidence interval coverage scores) and their corresponding standard
deviations were calculated by condition. These standard deviations were then
used to predict the Monte Carlo standard errors of the means of each outcome
measure across increasing sample sizes \parencite{siepe_simulation_2023}. These
Monte Carlo standard errors reflect the expected variation in the outcome
statistics due to random processes within the simulation. We selected the
number of data sets per condition for the full simulation, so that the maximum
expected Monte Carlo error across all outcome measures and conditions was 0.05.
This criterion was met with \textbf{N} data sets per condition.

\subsection{Model estimation}

After simulating the data, all introduced methods were applied to each data set
using the statistical software R \parencite{R-base}. First, the LPR was
estimated using the nprobust package \parencite{R-nprobust}, which allows to
correction for the bias inherent in LPRs. Second, GPs were estimated in STAN
\parencite{R-cmdstanr} with a zero mean and a squared exponential covariance
function, following common practice. Third, GAMs with a single smooth term for
time were fitted using the mgcv package \parencite{R-mgcv_a}. The polynomial
regressions were estimated using base R. Finally, the
parametric differential equation models corresponding to the true
data-generating models were estimated using the Dynr package
\parencite{R-dynr}. While the same non-parametric models were used across all
conditions, the parametric models were tailored to each specific
data-generating process. After fitting each model to the data, they were used
to obtain point and interval estimates (i.e., 95\% confidence and credible
intervals) for the latent process at each time point. A detailed description of
each model fitting procedure is provided in Appendix B.

To ensure reliable model fit and reasonable inferences, the fitting procedures
for each method were validated on pilot samples within each condition. After
this, an initial run of the simulation was performed, which revealed that the
GAMs over- or linearly underfit some data sets and that the parametric models
overfit some data sets. To prevent this, the fitting procedures of both methods
were adjusted and the simulation rerun. Further, if any models failed to
converge during the simulation, the corresponding outcome measures were
excluded from the following analyses.

\subsection{Outcome measures}

To evaluate and compare the performance of the different analysis methods, we
focused on three outcome measures. The first two assessed each method's
accuracy in predicting the process values at or between the observed time
points. These predictive accuracy measures indicate how well each method
captures the underlying non-linear process. The third outcome measure evaluated
the accuracy of the uncertainty estimates provided by each method.
Specifically, whether the confidence or credible intervals produced by each
method correctly included the true state value the expected proportion of
times.

\subsubsection{Capturing the non-linear process}

To assess how effectively each method captured the non-linear process at the
observed time points, we calculated the mean squared error (MSE) between the
estimated and generated process values. Additionally, to evaluate how well each
method would predict omitted process values within the process range, we
computed the generalized cross-validation
\parencite[GCV;][]{golub_generalized_1979} criterion for each method and data
set. The GCV is a more computationally efficient and rotation-invariant version
of the ordinary leave-one-out cross-validation criterion, with the same
interpretation. Both of these metrics describe how accurately the model
predicts omitted data points within the design range, which provides
information
of how well the model interpolates the process. The leave-one-out
cross-validation is calculated by removing one data point and refitting
the model while keeping certain parameters fixed (e.g., the bandwitdth of the
LPR, the smoothing penalty weight of the GAMs, and the covariance function
hyperparameters of the GP). The left out observation is then predicted by
the refit model and the squared prediction error is obtained. This procedure
is repeated for each data point and averaged. The GCV exploits the structure of
the used models to obtain the prediction errors for obtimally rotated data
without refitting the models \parencite{golub_generalized_1979}. Due to this
computational efficiency it is optimal for the use in large scale simulation
studies.

To analyze whether the simulated samples indicate true differences in the
MSE and GCV values across the different processes, analysis methdods, and
simulation conditions (i.e., sampling period, frequency, and dynamic error
variance) two seperate ANOVAs were fit. Both ANOVAs included all possible main
and interaction effects. However, the parametric models were excluded from
the ANOVAs as they were expected to outperform and hinder the comparison
of the other methods. The fitted ANOVAs were used to identify which effects
were significant and have at least a small effect size according to the partial
$\eta^2$ (> 0.01). Since, two seperate ANOVAs were fit, the resulting p-values
were adjusted for multiple comparisons by multiplying them by two. The
assumptions of the ANOVAs were tested. However, given the large sample sizes in
this simulation, the ANOVAs were assumed to be robust to moderate violations of
normality \parencite{blanca_non-normal_2017}, and any potential violations of
homoscedasticity were addressed using heteroscedasticity-consistent standard
errors.

\subsubsection{Uncertainty quantification}

To evaluate the uncertainty estimates provided by each method, we recorded
whether the true generated process was located within the confidence or
credible intervals at each time point. Subsequently, the average confidence
interval coverage proportion for each method and data set was obtained, by
averaging over all time points. Given that all confidence or credible intervals
were set at a 95\% confidence level, the expected coverage proportion should
ideally be close to 95\%. Due to Monte Carlo error in the simulation, average
coverage proportions between 93\% and 97\% were also deemed acceptable. Average
coverage proportions above 97\% suggested overestimated standard errors, while
those below 93\% indicated either a poor approximation of the underlying
process or an underestimation of the standard errors.

\subsection{Results}

In the simulation some of the GP regressions and parametric models
did not converge. This is most likely due to the small sample sizes considered
and the automated model fitting in the simulation. Most notably, the parametric
models were not able to infer the cusp catastrophe from the small simulated
data sets due to the complexity of the model. Figure~\ref{fig:missing} shows
the proportion of data sets for which each method did not converge in each
of the simulated conditions. The perfomrance measures of the methods that
did not converge were removed from the following analysis.

\begin{figure}[!t]
  \caption{Proportion of non-converged analysis in the simulation}
  \fitfigure{complete_results_missing.png}
  \label{fig:missing}
  \figurenote{This figure shows the proportion of analysis that did not
    converge during simulation in each condition.}
\end{figure}

\subsubsection{Capturing the non-linear process}

First, it is noteworthy that all considered methods visually performed well in
mean predicting the simulated processes. Figure~\ref{fig:smooth} illustrates an
example from each process being inferred by all methods. The predicted means
produced by each method closely followed the simulated processes, although the
LPR appears to underfit for some data sets. Additionally, near the boundaries
(i.e., the ends) of the simulated time series, the GP regression sometimes
tends towards zero, which is a characteristic of the squared exponential kernel
(this can be seen by comparing the GP inference in Figure~\ref{fig:gp_dem} to
the estimates in Figure~\ref{fig:gam_dem} or~\ref{fig:locpol_dem}). Further,
the polynomial regression appears to overfit near the boundary, resulting in
excessive uncertainty, which is a known behavior of polynomial regressions.
However, there is considerable variation and overlap in the accuracy of the
different methods across the various data sets, which highlights the need for a
more formal and objective analysis of the performance of each method.

\begin{figure}[!t]
  \caption{Example process inference by each of the introduced methods}
  \fitfigure{smooth.png}
  \label{fig:smooth}
  \figurenote{This figure shows how each of the introduced methods inferred an
    example of each of the processes from the simulation.}
\end{figure}

To achieve this, two separate ANOVAs were fitted to the MSE and GCV values,
including all possible main and interaction effects. Although the residuals for
both models showed considerable deviations from normality, which were mainly
characterized by being leptokurtic, the residuals were unimodal and
approximately symmetric. Given the large sample sizes in this simulation, we
thus assumed that the ANOVAs were robust to these deviations. Further, a
Breusch-Pagan test indicated heteroscedasticity in the residuals for both
outcome measures, which was corrected for using heteroscedasticity-consistent
standard errors. Laslty, Bonferroni corrections were applied to adjust the
p-values for conducting two separate ANOVAs.

The type-III ANOVAs for both outcome measures indicated that all main and first
order interaction effects were significant. Additionally, some higher order
interaction terms were significant, which differed between the two outcome
measures. However, due to the large sample size, even very small effects can
lead to statistical significance. Therefore, Table~\ref{tab:peta} presents all
effects for which the partial-$\eta^2$ (the proportion of variance explained by
an effect after accounting for all other effects) indicates at least a small
effect size for either the MSE or the GCV\@. The following sections will focus
on describing these effects and a comprehensive overview of all effects in the
models is can be in Appendix C.

\begin{table}[tbp]

  \begin{center}
    \begin{threeparttable}
      \caption{Effectsizes from the MSE and GCV ANOVAs}
      \label{tab:peta}
      \begin{tabular}{lll}
        \toprule
        Effect             & \multicolumn{1}{c}{partial-$\eta^2$ MSE} &
        \multicolumn{1}{c}{partial-$\eta^2$ GCV}
        \\
        \midrule
        Method             & 0.44                                     & 0.22 \\
        Process            & 0.51                                     & 0.41 \\
        SP                 & 0.09                                     & 0.04 \\
        SF                 & 0.23                                     & 0.33 \\
        DEV                & 0.55                                     & 0.52 \\
        Method:Process     & 0.16                                     & 0.06 \\
        Method:SP          & 0.10                                     & 0.04 \\
        Process:SP         & 0.03                                     & 0.02 \\
        Method:SF          & 0.05                                     & 0.01 \\
        Process:SF         & 0.02                                     & 0.07 \\
        Method:DEV         & 0.13                                     & 0.06 \\
        Process:DEV        & 0.26                                     & 0.25 \\
        SP:DEV             & 0.02                                     & 0.01 \\
        SF:DEV             & 0.01                                     & 0.09 \\
        Method:Process:SP  & 0.03                                     & 0.02 \\
        Method:Process:SF  & 0.02                                     & 0.01 \\
        Method:SP:SF       & 0.01                                     & 0.00 \\
        Method:Process:DEV & 0.05                                     & 0.02 \\
        Method:SP:DEV      & 0.01                                     & 0.01 \\
        Process:SF:DEV     & 0.01                                     & 0.03 \\
        \bottomrule
      \end{tabular}
      \tablenote{This table shows all effects from the MSE and GCV ANOVA that
        had at least a small effect partial-$\eta^2 >= 0.01$ on either
        outcome. SP\@: Sampling period; SF\@: Sampling frequency; DEV\@:
        Dynamic error variance.}
    \end{threeparttable}
  \end{center}

\end{table}

Figure~\ref{fig:mean_results_mse}~(a) illustrates the mean MSE values with
which each method inferred each process, averaged across sampling periods,
frequencies, and dynamic error variances. This figure shows the main effect of
the analysis method, as there are clear differences in the mean MSE with which
each method inferred all processes. Specifically, the parametric modelling
showed the lowest average MSE, followed closely by the GAMs and polynomial
regression. Additionally, Figure~\ref{fig:mean_results_mse}~(a) illustrates the
main effect of the processes, since there are differences in the mean MSE
values with which each process was inferred by all the methods. Most notably,
the cusp catastrophe was inferred with lower MSE values than the other
processes by all methods. Finally, one can see that there is an interaction
between the analysis methods and the processes, since the differences in how
accurately each process was inferred differ between the methods. For example,
the difference between the MSEs for the cusp catastrophe and the MSEs for the
other processes is larger for the LPR than for the other considered methods.

\begin{figure}[!t]
  \caption{Mean MSE effects}
  \fitfigure{mean_results_mse.png}
  \figurenote{Panel (a) shows the effect of the analysis method for each
    process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_mse}
\end{figure}

Figure~\ref{fig:mean_results_mse}~(b) shows the average MSE over different
measurement periods for each analysis method and process, averaged over
measurement frequencies and dynamic error variances. The results indicate that
sampling over the entire process, rather than just the first half, led to
higher average MSE values for both local and global polynomial regression
across all processes except the cusp catastrophe. This effect was much less
pronounced for the GP regression and GAMs, and was reversed for the parametric
models. Further, Figure~\ref{fig:mean_results_mse}~(c) illustrates that the
mean MSE generally decreased with larger sampling frequencies for each method
and process, while averaging ove the sampling periods and dynamic error
variances. Lastly, Figure~\ref{fig:mean_results_mse}~(d) demonstrates that
larger dynamic error variances increased the mean MSE values across all methods
and processes, when averaged over sampling periods and frequencies. However,
this effect was least pronounced for the cusp catastrophe.

Figure~\ref{fig:mean_results_gcv} displays the corresponding effects for the
mean GCV values. Similar to the MSE results, the GAMs show a mean GCV value
closest to the benchmark parametric models. However, while the polynomial
regression had lower mean MSE values than the GP regression across all
conditions, the GP regression had a lower mean GCV across all conditions. This
suggests that while the polynomial regression was more accurate at inferring
the underlying processes at the measurement points, the GP regression was more
accurate at interpolating omitted data points (while keeping the parameters of
the covariance function fixed). The effects of measurement period, frequency,
and dynamic error variance on the mean GCV appear to follow the same patterns
as observed for the mean MSE\@.

\begin{figure}[!t]
  \caption{Mean GCV effects}
  \fitfigure{mean_results_gcv.png}
  \figurenote{Panel (a) shows the effect of the analysis methods for each
    latent process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_gcv}
\end{figure}

\subsubsection{Uncertainty quantification}

Figure~\ref{fig:mean_results_ci_coverage} shows the average confidence interval
coverage proportion for the conditions described above. The gray area
represents an average confidence interval coverage between 93\% and 97\%. The
parametric models produced confidence intervals closest to this range, followed
by the GAMs and the polynomial regression.

\begin{figure}[!t]
  \caption{Average confidence interval coverage across all processes, analysis
    methods, and simulation conditions}
  \fitfigure{mean_results_ci_coverage.png}
  \figurenote{Panel (a) shows the effect of the analysis methods for each
    latent process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_ci_coverage}
\end{figure}

\subsection{Conclusion}

%Rewrite
This simulation showed that the GAMs inferred the latent processes more
accurately than the LPR and GP across all conditions, as indicated by the MSE,
GCV, and confidence interval coverage. Furthermore, the GAMs were nearly as
accurate as the true data-generating parametric models in the simulation. This
result was unexpected, as we anticipated that the smooth estimates produced by
GAMs might not be well-suited for inferring the rough (i.e.,
non-differentiable) processes in the simulation. However, the LPR and the
default GP configurations that were used also produce smooth estimates, leading
to similar misspecifications as the GAMs. This suggests that the accuracy of
the GAMs might have been least affected by the roughness of the latent process.
However, another factor potentially influencing the accuracy of the LPR and GP
is their assumption of constant wiggliness, as defined by their respective
bandwidth and lengthscale parameters, which was violated in the tested
processes.

It is important to note that the observed results may be attributable to the
specific configurations used for each method rather than the methods
themselves. These configurations were chosen to reflect how each method is
commonly applied in its most basic form and not to optimally infer the types of
processes simulated. Consequently, different configurations and extensions
might improve the performance of the LPR and GP\@. Importantly, it has been
shown that the standard GAM formulation is a (potentially improper) GP
\parencite{wahba_improper_1978}, and the specific GAM used in this simulation
can be expressed as a GP with a linear mean and a non-stationary covariance
function \parencite{rasmussen_gaussian_2006}. This implies that there are
certain GP formulations which perform at least as well as the GAMs did,
although these formulations deviate from how GPs are most commonly applied.
Regarding the LPR, several optimality results have been obtained for
differentiable processes, indicating that LPR should be at least as accurate as
the GAMs and GPs for these processes \parencite{fan_local_1997}. It is however
unclear whether these results generalize to the non-differentiable processes
that we suspect can be found most often in psychology.

Contrary to our expectations, the simulation indicated that the cusp
catastrophe process was inferred most accurately by the LPR, GP, and GAM\@. We
had anticipated that the smooth, continuous estimates produced by these methods
would struggle to adapt to the apparent jumps exhibited by this process.
However, this effect seems to have been mitigated by the cusp catastrophes
strong resilience to external perturbations. This property is highlighted in
Figure~\ref{fig:exemplar_pn}, where dynamic errors with the same variance have
been applied to all four processes. Despite the perturbations having an equal
variance, the cusp-catastrophe model appears to be the least affected and even
closely resembles the unperturbed process (Figure~\ref{fig:examplar_npn}).
Further evidence of this can be seen in the simulation, where the effect of
increasing the dynamic error variance was weakest for the cusp process. Due to
this, the simulation was rerun with considerably smaller dynamic error
variances, and under these conditions, the cusp model was indeed inferred with
the least accuracy.

% Still need to add citations to this paragraph
The results indicate that measuring at a higher frequency increased the
inference accuracy of all considered methods. Therefore, it is generally
advantageous from a statistical point of view to measure as frequently as
possible. However, in practice, this must be balanced against considerations
such as participant burden and fatigue, which can adversely affect data quality
if measurements are taken too often. Similarly, when selecting the sampling
period, it is essential to use domain knowledge about the scale of the
underlying dynamics to ensure that the measurements capture sufficient
variation in the latent process. Beyond this, the simulation showed that
extending the sampling period improved the inference accuracy of the GAMs and
parametric models but may decrease the accuracy of the LPR and GP\@. This
reduction in accuracy could however possibly be mitigated by using extensions
for a variable bandwidth, polynomial degree, or lengthscale respectively.
Lastly, the simulation revealed that larger dynamic error variances decreased
the accuracy of all methods. Therefore, reducing the magnitude of dynamic
errors is advisable in practice. This could, for example, be achieved by
measuring context variables and other sources of perturbations and
incorporating them into the model.

\section{An Empirical Example} \label{empirical_example}

In the following, the three analysis methods previously introduced were applied
to depression data from the Leuven clinical study. This study used experience
sampling measures to study the dynamics of anhedonia in individuals with major
depressive disorder \parencite{heininga_dynamical_2019}. This study was
selected for its heterogeneous sample, which includes participants with major
depressive disorder, borderline personality disorder, and healthy controls.
This diversity increases the likelihood of the data exhibiting a range of
(possibly non-linear) dynamics and processes. Specifically,
\textcite{houben_relation_2015} found in their meta-analysis that individuals
with lower psychological well-being tend to experience greater emotional
variability, less emotional stability, and higher emotional inertia. Although,
this finding did not replicate in an analysis of positive affect within the
Leuven clinical study \parencite{heininga_dynamical_2019}. Further, emotional
inertia, the extent to which an emotional state carries over across time
points, has been shown to vary within individuals over time (citation), which
makes it likely that the processes underlying this data are non-stationary.

To maintain consistency with how the methods were introduced and to avoid using
measurement models with multiple indicators, we analyzed momentary depression,
which was measured using a single item. This item was chosen over affect
measures because it displays sufficient variability, has a relatively low
proportion of participants with strong floor or ceiling effects, and is
measured on a broad response scale (0 to 100), making it ideal for illustrating
the introduced methods.

\subsection{Sample and data description}

The participants in the clinical sample of the Leuven clinical study were
screened by clinicians during the intake in three Belgian psychiatric wards
\parencite{heininga_dynamical_2019}. Patients who met the DSM criteria for mood
disorders or borderline personality disorder during the intake were eligible
for enrollment, while those presenting with acute psychosis, mania, addiction,
or (neuro-)cognitive symptoms were excluded. For a more thorough sample and
data description, see \textcite{heininga_dynamical_2019}. The final data set
used for this analysis contained 77 participants in the clinical sample and 40
participants in the control sample, who were matched to the clinical sample by
gender and age, resulting in a total sample size of 117 \footnote{The original
  published data set contained one additional participant who was removed for
  this analysis since they had a depression score of zero across all
  assessments.}.

During the study, all participants completed a baseline assessment, followed by
seven days of semi-random EMA assessments, with 10 equidistant assessments per
day. However, the starting date of the EMA measures varied between people.
During each assessment, participants responded to 27 questions covering
emotions, social expectancies, emotion regulation, context, and psychiatric
symptoms. This analysis focused on the item assessing momentary depressive mood
(i.e., `How depressed do you feel at the moment?') rated on a scale from 0 to
100.

The published data set was obtained from the EMOTE database
\parencite{kalokerinos_emote_nodate}. The initial study procedure was approved
by the KU Leuven Social and Societal Ethics Committee and the KU Leuven Medical
Ethics Committee. This secondary data analysis was approved by the Ethics
Review Board of the Tilburg School of Social and Behavioral Sciences (TSB RP
FT16).

\subsection{Analysis Plan}

The LPR, GP, and GAM were applied to explore the idiographic latent processes
underlying the data. Each method was applied separately to the time-series of
each participant, using the same specifications as in the simulation study
(Appendix B). However, for the LPR, only local cubic polynomials were
considered to keep the interpretation of the bandwidth consistent across
participants. Since all participants were assessed over seven days, but not
during the same period, the time series for each participant was centered so
that the first measurement time point served as the zero point. The LPR
bandwidth, GP lengthscale, and GAM smoothing parameter were then analyzed to
assess the wigglyness of the idiographic processes. Additionally, the GCV
values produced by each method were evaluated to determine which method
provided the most accurate interpolations. Lastly, the mean squared error was
calculated for each method and data set to estimate the expected measurement
error.

\subsection{Results (Template)}

The LPR, GP, and GAM were used to estimate the individual latent depression
processes. For the local cubic regression, the median optimal bandwidth was
21.28 hours (\textit{IQR}: 5.52). For the GP, the median optimal length scale
was 22.57 standard deviations (\textit{IQR}: 15.09). Lastly, for the GAMs, the
median optimal smoothing parameter was $8.18*10^9$ (\textit{IQR}:
$1.76*10^{10}$). These three measures of wiggliness are not only on different
scales, but there is also only a moderate correlation between the bandwidth and
lengthscale parameters (\textit{r} = 0.33). Further, the smoothing parameter of
the GAMs shows little to no correlation with the other two measures (bandwidth:
\textit{r} = -0.03; length scale: \textit{r} = -0.08). This discrepancy arises
because, while all three parameters reflect the wiggliness of the estimate,
they capture different aspects of it. For example, in data with a linear trend,
the bandwidth of the local cubic regression and the smoothing parameter of the
GAMs would theoretically be infinite, while the length scale parameter of the
GP would have a finite value. Additionally, the interpretation of each
wiggliness parameter depends in the model configurations chosen and would
change for different configurations of these methods.

Because of this, there is not much value in interpreting the absolute values of
these parameters. Instead, we explored the range of functional behaviors
inferred by the most extreme values of each parameters. Figure
\ref{fig:dem_smooth} shows the ten least and most wiggly processes inferred by
each method. This figure reveals considerably heterogeneity in the functional
behavior inferred by each method. Most interestingly, the least wiggly
processes inferred by both the GPs and the GAMs are linear trends, indicating
the absence of any dynamic errors for these individuals. In contrast to this,
the processes with the highest inferred wigglyness, display either large
dynamic errors around their respective person means or in addition to this a
different non-linear dynamic.

\begin{figure}[!t]
  \caption{The ten least and most wiggly idiographic latent depression
    processes as inferred by the LPRs, GPs, and GAMs}
  \fitfigure{demonstration_smooths.png}
  \label{fig:dem_smooth}
\end{figure}

Lastly, a cross-validation was conducted using the generalized cross-validation
criterion to investigate which method predicted the latent processes most
accurately. The median GCV for the GAMs was 125.29 (\textit{IQR}: 201.04), for
the GP it was 248.27 (\textit{IQR}: 578.00), and for the LPR it was 131.57
(\textit{IQR}: 213.73). In addition to this, the mean squared error was
calculated between the predictions generated by each method and the data. Here
the median MSE of the GAM was 114.04 (\textit{IQR}: 171.59), for the GP it was
197.54 (\textit{IQR}: 388.40), and for the LPR it was 101.74 (\textit{IQR}:
153.77). Together with the GCV this indicates that the GAM inferred the latent
processes most accurately, whereas the LPR slightly overfit the data and the
GPs tended to underfit the data.

\section{Conclusion}

\textit{WIP}

\section{Discussion}

% Rewrite into ---  

This article has several important limitations. Since, only a limited selection
of processes was used it is possible that the presented results may not
generalize to other processes. However, the used processes already constitute
violations to the smoothness assumption made by the LPR, GP, and GAM to which
these methods demonstrated some robustness. In addition to this, as explained
earlier, using different configurations for the LPR, GP, and GAM is likely to
change the presented results. Further, we focused on introducing these methods
by inferring time dependent non-linear processes from univariate, single
subject data with independent normally distributed measurement errors. This is
a statistically idealized setting, which does not address many of the goals and
challenges researchers are facing when working with ILD\@. Frequently, ILD
contains measurements for many individuals on several psychological constructs.
Classically, this enables researchers to study how these constructs vary and
interact over time and how these dynamics differ between people. In addition to
that, each construct is frequently measured using multiple indicators with
ordinal measurement errors, which are modelled using different psychometric
models (e.g., factor models, item response models).

There are fortunately many ways in which the presented methods can be adapted
to these more complex data structures. The GP and GAM can be easily adapted to
work with multilevel data without substantially extending the statistical
theory underlying both methods. For the GAM this extension is already
implemented in some software. Another approach may be to study between person
differences in the latent processes using functional data analysis. In this
analysis the individual latent processes are first estimated using one of the
presented data driven techniques. Subsequently, the inferred processes are
treated as function valued data, which can be analyzed to find for example
group differences in a functional ANOVA \parencite{kaufman_bayesian_2010} or to
find the functions which account for the maximum between person variation in a
functional principal component analysis \parencite{aue_prediction_2015}.

Similarly, the GP and GAM are already equipped to estimate latent variables in
general and they are not limited to single indicators. Therefore, it is
possible to extend them to incorporate typical psychometric measurement models,
to accurately capture more complex measurement error distributions.
Unfortunately, the software implementations of this may be computationally very
intensive and are currently only available in a very limited form
\parencite{clark_dynamic_2023}. The parametric models introduced already
include a factor model for the observed variables, which can incorporate
multiple indicators. In addition to this, these indicators can also be
non-normally distributed.

Lastly, there are many ways in which the presented methods can be used to study
multivariate data. This is because, even though all methods were used to infer
time dependent non-linear processes in this paper, they can in theory be used
to estimate many smooth and continuous functions (and even non-smooth and
non-continuous functions to the degree presented in this paper). This makes it
possible to for example infer non-linear cross- and autoregressive
relationships from data in discrete time \parencite{wood_generalized_2006,
  rasmussen_gaussian_2006, eleftheriadis_identification_2017} and non-linear
differential equation models in continuous time
\parencite{yildiz_learning_2018}. For the presented processes in particular
this should even be more appropriate, since they were generated using
differential equation models. These models also present the exciting
possibility to combine partial parametric models with non-linear data driven
functions. Another possibility is to infer an unobserved input variable to a
parametric differential equation model
\parencite{alvarez_latent_2009,nayek_gaussian_2019}. Lastly, it is possible to
infer non-linear seasonality and cyclic components on indicator variables
\parencite{clark_dynamic_2023}.

\printbibliography[]

\end{document}