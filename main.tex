% -----------------------------------------------------------------------------%
% Title:                                                                       %
% Author: Jan Ian Failenschmid                                                 %
% Created Date: 13-03-2024                                                     %
% -----                                                                        %
% Last Modified: 15-08-2024                                                    %
% Modified By: Jan Ian Failenschmid                                            %
% -----                                                                        %
% Copyright (c) 2024 by Jan Ian Failenschmid                                   %
% E-mail: J.I.Failenschmid@tilburguniveristy.edu                               %
% -----                                                                        %
% License: GNU General Public License v3.0 or later                            %
% License URL: https://www.gnu.org/licenses/gpl-3.0-standalone.html            %
% -----------------------------------------------------------------------------%

\documentclass[man, floatsintext]{apa7}

% Dependencies
\usepackage{csquotes, amsmath, amssymb, mathptmx, enumitem}
\usepackage[american]{babel}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\graphicspath{ {./figures} } % Gaphics Path
\addbibresource{bibliography.bib} % Literature bibliography
\addbibresource{R_bib.bib} % R bibliography

% Title Page
\title{Modeling Non-Linear Psychological Processes: Reviewing and Evaluating
  (Non-) parametric Approaches and Their Applicability to Intensive
  Longitudinal
  Data}

\shorttitle{Modelling Non-Linear Psychological Processes}

\leftheader{Failenschmid}

\authorsnames{{Jan I. Failenschmid}, {Leonie V.D.E Vogelsmeier}, {Joris
      Mulder}, {Joran Jongerling}}

\authorsaffiliations{{Tilburg University}}

\abstract{Here could your abstract be!}

\keywords{Here could your keywords be!}

\authornote{
  \addORCIDlink{Jan I. Failenschmid}{0009-0007-5106-7263}

  Correspondence concerning this article should be addressed to Jan I.
  Failenschmid, Tilburg School of Social and Behavioral Sciences: Department of
  Methodology and Statistics, Tilburg University, Warandelaan 2, 5037AB
  Tilburg,
  Netherlands.	E-mail: J.I.Failenschmid@tilburguniversity.edu}

\begin{document}

\maketitle

Psychological constructs are increasingly recognized as components of complex
dynamic systems \parencite{nesselroade_studying_2004, wang_investigating_2012}.
This perspective emphasizes how these constructs fluctuate over time and within
individuals. To study these variations and the underlying processes,
researchers are increasingly collecting intensive longitudinal data (ILD) using
ecological momentary assessment (EMA), experience sampling, or similar methods
\parencite{fritz_so_2023}. These studies assess several individuals at high
frequencies (up to multiple times per day) using brief questionnaires or
passive measurement devices. This rich data allows researchers to examine
variations in latent psychological variables within an ecologically valid
context and to explain them through (between-person differences)
in within-person processes (citation).

Many psychological phenomena and processes have been shown to be non-linear,
leading many researchers to believe that the underlying systems likely exhibit
similar complexity. Clear examples of this are the learning and growth curves
observed in intellectual and cognitive development
\parencite{kunnen_dynamic_2012, mcardle_comparative_2002}. In these cases, an
individual's latent ability increases over time from a (person specific)
starting point toward an (person specific) asymptote,
which reflects their maximum ability. Additionally, examples of
asymptotic growth over the shorter time spans that are typically studied with
ILD include
motor skill development \parencite{newell_time_2001} and second language
acquisition \parencite{de_bot_dynamic_2007}. Figure~\ref{fig:examplar_npn}
illustrates general growth curves through an exponential growth function (a)
and a logistic growth function (b), both of which are common model choices for
these processes.

\begin{figure}[!ht]
  \caption{Examples of non-linear processes demonstrated to occur in
    psychological time series.}
  \fitfigure{exemplar_no_process_noise.png}
  \figurenote{This figure shows four demonstrated psychological non-linear
    processes. Panels (a) and (b) show exponential and logistic growth curves,
    respectively, which have been shown to describe intellectual and cognitive
    development \parencite{kunnen_dynamic_2012,mcardle_comparative_2002},
    motor skill learning \parencite{newell_time_2001}, and second language
    acquisition \parencite{de_bot_dynamic_2007}. Panel (c)
    shows a cusp model from catastrophe theory that exhibits apparent jumps
    between two stable states. As such, it has been used to describe the
    perception of mental flow \parencite{ceja_suddenly_2012} or alcohol use
    relapse \parencite{witkiewitz_modeling_2007}.
    Lastly, panel (d) shows a damped oscillator, which describes
    the return of a perturbed system to baseline, which has been proposed as a
    model for affect regulation \parencite{chow_emotion_2005}.}
  \label{fig:examplar_npn}
\end{figure}

Another commonly observed non-linear phenomenon involves the construct under
study switching between multiple distinct states, which often reflect different
means. This occurs, for example, during the sudden perception of cognitive
flow, where individuals abruptly switch from a `normal' state to a flow state
and back \parencite{ceja_suddenly_2012}. Another example is alcohol use
relapse, where patients suddenly switch from an abstinent state to a relapsed
state \parencite{witkiewitz_modeling_2007}. This sudden switching behavior is
exemplified in Figure~\ref{fig:examplar_npn}~(c)
through a cusp catastrophe model. This model, drawn from catastrophe
theory, naturally leads to mean level switches when varying one of its
parameters \parencite{van_der_maas_sudden_2003,chow_cusp_2015}.

As a final example, one may consider (self-) regulatory systems, which maintain
a desired state by counteracting perturbations. These systems regulate
adaptively, meaning that the regulation strength depends on the distance
between the
current and desired states. The common autoregressive model describes such a
system with a linear relationship between this distance and regulation
strength. However, this relationship may also be non-linear, such that the
regulatory force increases disproportionately with larger mismatches. Such
models have been proposed to describe for example emotion regulation
\parencite{chow_emotion_2005}. \textit{An example involving depression could be
  included here, but a suitable reference has not yet been found.}
Figure~\ref{fig:examplar_npn}~(d) illustrates a
(self-) regulatory system exemplified as a damped oscillator.

Although initial evidence for non-linearity in psychological research exists,
theories about the nature and form of non-linear psychological processes remain
scarce \parencite{tan_time-varying_2011}. This gap is largely due to the lack
of advanced statistical methods that are flexible enough to study the complex
behavior of these processes adequately. As a result, researchers are often
ill-equipped to infer the functional characteristics of non-linear processes
from ILD, which hinders the development and evaluation of guiding theories for
subsequent studies. Due to this lack of adequate available statistical methods,
non-linear trends are most often addressed in psychology through polynomial
regression or regression splines.

Polynomial regression \parencite{jebb_time_2015} uses higher-order terms
(e.g., squared or cubed time) as predictors in a standard multiple linear
regression model. While effective for relatively simple non-linear
relationships,
particularly those that can be represented as polynomials, this method has
significant limitations and likely leads to invalid results when applied to
more complex latent processes, such as mean switching or (self-) regulatory
systems (e.g., Figure~\ref{fig:examplar_npn}~c~\&~d). In these cases,
polynomial approximations require many higher-order
terms to capture the process's high variability, which raises the problem of
over- or underfitting the data, causes model instability, and leads to
nonsensical inferences (e.g., interpolating scores outside the scale range;
\textcite{boyd_divergence_2009,harrell_general_2001}).

An alternative approach is spline regression, which constructs a complex
non-linear trend by joining multiple simple piecewise functions at specified
points, known as knots (e.g., combining multiple cubic functions into a growth
curve with plateaus; \textcite{tsay_nonlinear_2019}). However, spline
regression requires careful selection of the optimal piecewise functions and
knot locations. This can be problematic in practice because, as mentioned,
precise guiding theories about the functional form of most psychological
processes are lacking \parencite{tan_time-varying_2011}. This absence of clear
guidance can easily lead to misspecified models and invalid results.

These limitations in current practices underscore the need for alternative
statistical methods to study non-linear processes. Various
such advanced statistical
methods, such as kernel regression, Gaussian processes, smoothing splines, and
latent change score models, are available outside of psychology. However, these
methods have rarely been applied in psychology because they have not been
reviewed for an applied audience, nor have their assumptions and inference
possibilities been evaluated in the context of ILD.\@ As a result,
psychological researchers struggle to select the most suitable method for a
specific context. This challenge is further complicated by the fact that the
ideal statistical method may depend on the characteristics of the underlying
non-linear process, which are generally unknown. Additionally, the smooth
processes for which many of these methods were originally developed are
unlikely to occur in psychological research.

To address this important gap, this article reviews four advanced non-linear
analysis
methods and evaluates their applicability to typical ILD.\@ Specifically, we
compare how well each method can recover different latent processes under
typical ILD conditions through a simulation study. We also demonstrate the
conclusions that can be drawn from each method by applying them to an existing
dataset. The methods reviewed in this article range from data-driven
non-parametric techniques to a flexible parametric modeling framework. These
approaches were selected to accommodate varying degrees of prior knowledge, as
precise theories about the nature of non-linear psychological processes are
scarce \parencite{tan_time-varying_2011}. Further, to introduce these methods
accessibly and apply them under conditions where software implementations
are available, this article focuses on the univariate single-subject design.

\section{Method}

\subsection{Data structure}

Generally, any psychological construct under study follows a (possibly
non-linear) function over time, as represented by the lines in Figure 1.
However, since these psychological processes are typically unobservable or
latent, they are measured through observable indicators, such as questionnaire
items or passive measurements. The observations of these indicators
(Figure~\ref{fig:examplar_npn}, dots)
differ from the true values of the latent process due to measurement
error, which may result from an imperfect measurement instrument.
For this introduction, we assume that all time-point-specific measurement
errors are independent and normally distributed. The model for the
observations of a single indicator can then be written as:

\begin{align}
  Y_t = f(t) + \epsilon_t; \quad \epsilon_t \sim N(0, \sigma^2_{\epsilon})
\end{align}

\noindent where $f(t)$ represents the potentially non-linear latent process,
and $\epsilon_t$ represents the time-point-specific measurement error.

In this context, researchers typically aim to infer the underlying process and
draw conclusions about its functional form. The following sections will
introduce four methods to achieve these goals, two non-parametric techniques,
one semi-parametric approach, and one parametric modeling framework, using the
(self-) regulatory process depicted in Figure~\ref{fig:examplar_npn}~(d)
as a running example.

\subsubsection{Local polynomial regression}

The first technique is called local polynomial regression (LPR). Similarly to
regular polynomial regression, LPR approximates the process using polynomial
basis functions (e.g., squared or cubed time). However, instead of using one
large polynomial to approximate the entire process, LPR estimates smaller,
local polynomials at each point in time. These local polynomials are then
combined into a single non-linear function over the entire set of observations
\parencite{fan_adaptive_1995, ruppert_multivariate_1994, fan_local_2018}.

To determine the value of the LPR at a specific time point, the data is
centered around that point (by shifting the data along the time axis so
that the chosen time point is at zero), and a low-order polynomial is fitted
around it. Additionally, to account for the idea that data points closer in
time are more related, a weighting function is applied during the polynomial
estimation. This function assigns weights to each data point based on its
distance from the point of interest. The intercept of this locally weighted
polynomial provides the value of the LPR for the chosen timepoint.
To find the LPR value at a different time
point, the same procedure is repeated, centering the data around the new point
of interest. The theoretical ability to repeat this process at infinitely many
time points makes LPR a non-parametric technique. Figure~\ref{fig:locpol_dem}
shows the estimated LPR for the example process depicted in
Figure~\ref{fig:examplar_npn}~(d).  In this figure, three truncated examples of
local cubic regressions that contribute to the LPR for this process are shown
in red.\@

\begin{figure}[!ht]
  \caption{Demonstration of how local polynomial regression (solid black)
    estimates the underlying process (dotted black). Here, three local cubic
    functions (red) are shown as examples at the time points 50, 100, and 150,
    which provide the values of the local polynomial regression at these time
    points.}
  \fitfigure{locpol_demonstration.png}
  \label{fig:locpol_dem}
\end{figure}

When fitting an LPR, several decisions must be
made regarding the degree of the local polynomials and the optimal weighting of
the data. Typically, the degree of these local polynomials is kept low and odd.
This choice reflects a bias-variance tradeoff, where higher-order polynomials
reduce bias but increase variance, when transitioning from an odd
to an even power (citation). Data weighting in LPR is achieved through a kernel
function, which is usually centered and symmetric, assigning weights based on
the distance from the origin. Common kernel choices include the Gaussian and
symmetric Beta distributions. Most commonly used kernel functions yield similar
inferences and a specific kernel can be selected to minimize a chosen criterion
function. The kernel is further defined by a bandwidth
parameter, which determines its width and effectively controls the influence of
more distant data points. The bandwidth parameter, in practice, represents the
wiggliness of the estimated process.
Several methods are available to find the optimal bandwidth by optimizing a
data-dependent criterion function, such as cross-validation or the mean
integrated squared error \parencite{kohler_review_2014, debruyne_model_2008}.

Due to its non-parametric nature, LPR makes minimal assumptions about the data.
However, it does require that the underlying process is smooth or
differentiable with regards to the covariate, which is
a necessary condition for polynomial approximation.
Another key assumption is that the process has constant
wiggliness, represented by a single bandwidth parameter. However,
this assumption can be relaxed by using a time-varying bandwidth.

\subsubsection{Gaussian process regression}

The second non-parametric technique is Gaussian process (GP) regression, a
Bayesian approach that directly imposes a probability distribution over an
entire family of non-linear functions flexible enough to capture many complex
processes effectively \parencite{rasmussen_gaussian_2006,
  betancourt_robust_2020, roberts_gaussian_2013}. Unlike regular probability
distributions (e.g., normal distributions) that specify the plausibility of
single values, Gaussian processes determine the plausibility of entire
(non-linear) functions. In a Bayesian framework, one can use a GP to define
a prior distribution for the latent process. This prior is then combined
with an appropriate likelihood for the observed data to obtain a posterior
distribution for the latent process given the observed data.

The posterior distribution represents an updated belief about which functions
describe the latent process well, allowing one to draw inferences about the
process. Figure~\ref{fig:gp_dem} illustrates such a posterior distribution
for the runnign
example process. The red lines represent a sample of non-linear functions
drawn from the posterior distribution, with the pointwise average of these
functions providing a mean estimate for the underlying process.

\begin{figure}[!ht]
  \caption{Demonstration of how Gaussian process regression estimates the
    underlying process (dotted black). Here, a sample of functions drawn from
    the
    posterior Gaussian process probability distribution is shown (red). The
    predicted value for the underlying process is then obtained by averaging
    the
    drawn functions.}
  \fitfigure{gp_demonstration.png}
  \label{fig:gp_dem}
\end{figure}

The GP prior is parameterized by a mean function and a covariance function,
which are continuous extensions of the mean vector and covariance matrix of a
multivariate normal distribution. These functions can be selected based on
domain knowledge or through data-driven model selection
\parencite{richardson_gaussian_2017, abdessalem_automatic_2017}. In practice,
the mean function is often set to zero when no specific prior knowledge is
available, which does not constrain the posterior mean to zero but instead
indicates a lack of prior information about its deviations from zero.
The covariance function is typically based on a kernel function, which assigns
covariances between time points only depending on their distance
(e.g., quadratic exponential, Matern class). Finally, appropriate hyperpriors
for the parameters of the mean and covariance functions are used to generate
the corresponding posterior distribution. These distributions reflect
prior beliefs about the hyperparameters and can be used to constrain them to
sensible values.

The functional behavior of a GP is entirely determined by the posterior mean
and covariance function. To accurately capture a process, it is crucial
that the functional familiy generated by a chosen mean and covariance function
is similar to the actual process.
Common choices for the covariance function result in smooth and
covariance-stationary GPs with constant wiggliness. Unlike LPR, GP
regression provides more interpretable information about the process through
the posterior distributions of hyperparameters.
One typical hyperparameter, the characteristic length scale of the covariance
function, is analogous to the bandwidth parameter in LPR, as it also describes
the wiggliness of the estimated process. However, GP regression can
include additional hyperparameters, allowing for more specific theories to be
tested through model comparison.

\subsubsection{Generalized additive models}

Generalized additive models (GAMs) are a semi-parametric modeling framework
that utilizes smooth terms, which are non-linear functions
that are inferred from the data through
smoothing splines \parencite{wood_generalized_2006, wood_inference_2020,
  hastie_generalized_1999}. Smoothing splines are an extension of regular
spline
regression, which circumvent the knot placement issue by placing a knot at each
observation \parencite{tsay_nonlinear_2019}. This approach is equivalent to a
basis function regression with as many basis functions as there are
observations. However, using this many knots would lead to overfitting. To
prevent this, smoothing splines use a penalty term, similar to the penalty
used in a lasso or ridge regression, which controls the smooth term's
wigglyness
\parencite{gu_smoothing_2013, wahba_spline_1980}. This penalty makes it
possible to balance the flexibility and overfitting of the smooth term,
ensuring that the model captures the process
accurately without excessive complexity. The optimal weight of the
penalty is
determined by minimizing a criterion function, such as the generalized
cross-validation criterion \parencite{wood_generalized_2006,
  golub_generalized_1997}. While researchers must still choose a set of basis
functions, cubic \parencite{tsay_nonlinear_2019} and thin plate spline bases
\parencite{wood_thin_2003} are optimal for many applications.

These smooth terms can then be combined in an additive regression model, where
each smooth term essentially functions as a predictor within a regular
regression
analysis. In this model, smooth terms can be multiplied by covariates and
summed into a single overall non-linear function to explain the data. This
approach enables the formulation of models such as a time-varying
autoregressive model, where the intercept and autoregressive parameters are
smooth functions that vary over time \parencite{bringmann_changing_2017,
  bringmann_modeling_2015}. By integrating non-parametric smooth terms into a
broader parametric model, GAMs become semi-parametric models that are
well-suited for testing specific hypotheses while maintaining the flexibility
needed to accurately capture the underlying process. Figure~\ref{fig:gam_dem}
illustrates a simple GAM with a single smooth term for time, fitted to the
example process. Some of the scaled thin-plate smoothing spline basis
functions that constitute the smooth term are shown in red.

\begin{figure}[!ht]
  \caption{Demonstration of how generalized additive models (solid black)
    estimate the underlying process (dotted black). Here, the predicted values
    for the process at any point in time correspond to the weighted average of
    the basis functions (red).}
  \fitfigure{gam_demonstration.png}
  \label{fig:gam_dem}
\end{figure}

Compared to previous methods, GAMs offer a more accessible modeling framework,
enabling the specific modeling and testing of partial theories. For instance, a
GAM can model a linear trend while adding a smooth term around it to capture
non-linear deviations. This makes it possible gain insight into both
the linear trend and the necessity of the smooth term, which can be examined
through model comparison.
Additionally, GAMs also provide an estimate of the wigglyness
of the process through the penalty weight term. In contrast, to LPR and GP
this penalty weight does not assume constant wigglyness. Lastly, the
basis function coefficients within each smooth term can be interpreted.
However, the specific interpretations depend on the spline basis used.

\subsubsection{Parametric models}

Finally, several methods to parametrically model non-linear processes will be
disucussed. The most direct approach for this involves defining a non-linear
regression model for the process itself. As with all parametric models,
this approach requires a thorough
prior theory about the functional form of the process. However, such theories
are often difficult or even impossible to find in practice due to the
assumed complexity of the underlying dynamic systems.

However, in the case of the running example, such a formulation exists.
Considering the oscillatory nature of this (self-) regulatory process,
a cosine function is a natural candidate to model it.
Additionally, since the oscillations diminish over time,
the model should account for the decreasing amplitude. An example of such a
model is:

\begin{equation}
  \begin{aligned}
    f(t)   & = A e^{-c t} \cos(\omega t - \delta) \\
    \omega & = \sqrt{k - \frac{c^2}{4}}
  \end{aligned}
\end{equation}

\noindent where the parameters control the initial amplitude of the wave
function ($A$),
the frequency ($\omega$), the rate at which the amplitude decreases ($c$), and
the phase ($\delta$). Even for this relatively simple system, this parametric
form is already quite complex. For more intricate processes, finding an
appropriate model of this form can be nearly impossible.

Instead it is often more practical to define a model for how a process changes
over
time using a differential equation model \parencite{cooper_dynamical_2012}.
These models describe the relationship between the current value of the process
and its instantaneous rate of change. By combining a model of the process's
change with information about its initial state, it is possible to infer the
entire trajectory of the process.
Differential equations form a general class of models capable of representing
various dynamic processes. For the running example, we use a damped oscillator
model. This is a classic differential equation model originally designed to
describe the
behavior of an oscillating spring with resistance. This model can be expressed
as:

\begin{equation}
  \begin{aligned}
    \frac{\partial f}{\partial t} & = v         \\
    \frac{\partial v}{\partial t} & = -cv + -kf
  \end{aligned}
\end{equation}

\noindent This model relates the rate of change of the position of the spring
$f$ to
its velocity $v$. In turn, the rate of change of the velocity is determined
by the velocity itself through the damping coefficient $c$ and by the
position of the spring through the spring constant $k$. Here, the spring
constant is related to the frequency of the oscillations since a stiffer
spring oscillates faster and the damping coefficient is related to how quickly
the spring returns to its basis position after being perturbed.
\textcite{chow_emotion_2005} show how this model can be applied in a
psychological context to model emotional self-regulation. Such that a perturbed
emotional state is regulated back to the desired baseline faster the larger
the perturbation is.

In this model, $f$ epresents the position of the spring, and $v$ denotes its
velocity, which is the rate of change of the position. The rate of change of
the velocity is influenced by both the velocity itself, through the damping
coefficient $c$ and the position of the spring, through the spring constant
$k$. The spring constant $k$ influences the frequency of oscillations, since a
stiffer spring oscillates faster, while the damping coefficient $c$
determines how quickly the spring returns to its equilibrium position after
being disturbed. \textcite{chow_emotion_2005} demonstrate how this model can be
applied to psychological contexts, such as emotional self-regulation. In their
application, a perturbed
emotional state is regulated back to the desired baseline faster the larger
the perturbation is.

Alternatively, when working with equally spaced time points, one can model
dynamic local changes in discrete time using difference equations
\parencite{durbin_time_2012}. A common example of this approach is the classic
AR(1) model. These equations express the current value of the latent construct
as a function of its previous value. While the smooth nature of the running
example process cannot be perfectly captured by difference equations, it can be
approximated by these equations:

\begin{equation}
  \begin{aligned}
    f_{t} & = f_{t-1} + v_{t-1}        \\
    v_{t} & = (1-c)v_{t-1} + -kf_{t-1}
  \end{aligned}
\end{equation}

Lastly, when modeling a process through local dynamics, two types of errors
should be taken into account. The first is dynamic error, which comprises
external perturbations
to the latent construct value that are carried forward over time. For instance,
if a participant experiences an unusually pleasant conversation that elevates
their true positive affect, this change represents an error effect if such
external influences are not accounted for by the model. However, since the true
positive affect level has increased, this will influence future measurements
due to emotional inertia. Dynamic errors can be incorporated into the model in
different ways, with the most common approach being the addition of a normally
distributed error term to the deterministic dynamic change models described
above.

The second type of error that should be considered is measurement error,
which represents the difference between observed measurements and the
latent construct values, which may be introduced by an imperfect measurement
instrument. These errors are typically
modeled using a factor or item response model that links the observations to
the latent construct. Combining a dynamic model with a dynamic error component
and a factor model yields a state space model \parencite[discrete
  time;][]{durbin_time_2012} or a stochastic differential equation model
(continuous time). These models can then be used to infer the process and
estimate the parameters of the dynamic equations using the Kalman filter and
its extensions \parencite{chow_unscented_2007}.

\section{Simulation}

\subsection{Problem}

To assess the effectiveness of the introduced
parametric and non-parametric statistical methods in recovering different
non-linear processes commonly encountered in experience sampling research
(Figure~\ref{fig:examplar_npn}) a simulation study was conducted.
Further, to apply these models under the conditions described in the
introduction,
and within the constraints of available software implementations, we focused on
a univariate single-subject design. Thus, the simulated data represented
repeated measurements of a single variable for one individual.

\subsection{Design and Hypotheses}

For the non-parametric analysis methods (i.e., LPR, GP), we expect that
each method in its default configuration will most accurately infer processes
that are (a) continuous (i.e., without sudden jumps), (b) have constant
wigglyness (i.e., constant second derivative), and (c) are smooth (i.e.,
differentiable). This expectation arises because both methods by default
produce
continuous, smooth estimates with a single constant bandwidth or lengthscale.
For the GAMs, we expect that only criteria (a) and (c) will influence
inference, as GAMs do not assume constant wigglyness. The
parametric modeling approach is expected to provide the most accurate
inferences, serving as a benchmark for comparison with the other methods. We
also expect that the performance of parametric models will decrease as the
model complexity increases (e.g., with jumps or reduced smoothness), though
likely less so than with non-parametric and semi-parametric models.
Additionally, we expect that larger sample sizes will lead to more accurate
inferences, with both (d) the overall length of the sampling period and (e) the
sampling frequency being varied.

To conduct the simulation with processes that might be encountered in real
EMA studies, we selected the exemplar processes illustrated in
Figure~\ref{fig:examplar_npn} as a basis.
These include two growth curves, modeled as an exponential and
a logistic growth function, a mean-level switching process, modeled as a cusp
catastrophe, and a self-regulatory process, represented by a damped oscillator.
These processes allow us to test the impact of (a) sudden jumps and
(b) changing wigglyness on the four methods.
First, we hypothesize that the cusp catastrophe model, which is the only
process featuring jumps, will be least accurately inferred by all methods.
Second, all four processes exhibit changes in wigglyness (i.e., changes in the
second derivative) over time. However, while the wiggliness of the exponential
and logistic growth functions and the damped oscillator decreases
monotonically, the cusp catastrophe's wigglyness changes cyclically.
Therefore, we hypothesize that longer sampling periods for the exponential and
logistic
growth curves and the damped oscillator will reduce the inference accuracy of
the LPR and the GP, as these rely on a single bandwidth or lengthscale
parameter, which may become increasingly inadequate in capturing the changing
wigglyness over time. We do not expect this effect to occur for the cusp
catastrophe process, or when using GAMs or parametric models.

To manipulate the (c) smoothness of the processes, a dynamic noise
component was added to the data-generating models. This perturbed the
processes at each point in time by a normally distributed error,
resulting in non-smooth (i.e., non-differentiable or rough) trajectories.
The degree of roughness was controlled
by the variance of these dynamic errors and we considered variances of
0.5, 1, and 2 reasonable relative to the process range.
Figure~\ref{fig:exemplar_pn}
illustrates one possible realization of the exemplar processes with dynamic
noise. Importantly, we intentionally omitted a condition without dynamic noise
from this simulation, as dynamic noise is reasonably expected to be present in
all psychological intensive longitudinal data (ILD).

\begin{figure}[!ht]
  \caption{Non-linear exemplar processes}
  \fitfigure{exemplar_process_noise.png}
  \label{fig:exemplar_pn}
\end{figure}

Additionally, the sample size was varied during the simulation by manipulating
both (d)
the sampling period and (e) the sampling frequency, as these methodological
choices are expected to impact the performance of the analysis methods
differently. Specifically, for the LPR and the GP, which rely only on data in
local neighborhoods, we expected that extending the sampling period beyond this
neighborhood will not increase the inference accuracy.
In fact, if the process exhibits
changing wigglyness over the extended period, as previously discussed,
increasing the sampling period might even negatively affect the inference
accuracy. In contrast, we expeced GAMs and parametric models, which incorporate
the entire dataset in their estimations, to perform better with a longer
sampling period. Since there is no inherent scaling to the time axis in this
simulation, we chose to simulate the first half of each process in one
condition and the full process in another, referred to as sampling periods of
one or two weeks. This scaling is arbitrary and could be changed to any time
frame. Lastly, we expected that increasing the sampling frequency will
generally improve the inference accuracy across all methods, as it provides
more information about the latent processes.
We tested sampling frequencies of three, six, and nine measurements per day,
to align these choices with typical experience sampling study designs
\parencite{wrzus_ecological_2023}.

\subsection{Procedure}

To simulate the data, each exemplar process was represented as a generative
stochastic differential equation. This means that, at each time point,
the rate of change of the
process was determined by a nonlinear function of its current value together
with an
additive dynamic error component modeled by a Wiener process. Data from these
generative models were then simulated using the Euler-Maruyama method, ensuring
accurate dynamic error variance, sampling frequency, and sampling period for
each condition. Measurement errors were added to the latent process data at
each time point using a standard normal distribution, generating the final sets
of observations. For a detailed technical explanation of the data generation
process, see Appendix A.

To determine the required number of data sets per condition, a
power simulation was conducted
based on an initial pilot sample of 30 generated data sets per
condition. Based on these data sets, the outcome measures
(e.g., MSE, GCV, and confidence interval coverage scores) and their
corresponding standard deviations were calculated by condition.
These standard deviations were then used to
predict the Monte Carlo standard errors of the means of each outcome measure
across increasing sample sizes. Monte Carlo standard errors reflect the
expected variation in an outcome statistic due to random processes within the
simulation. We selected the number of data sets per condition for the full
simulation, so that the
maximum expected Monte Carlo error across all outcome measures
and conditions was 0.05. This criterion was met with \textbf{N} data sets per
condition.

\subsection{Model estimation}

After simulating the data, all introduced methods were applied to each data set
using the statistical software R \parencite{R-base}. First, the LPR was
estimated using the nprobust package \parencite{R-nprobust}, which allows to
correction for the bias inherent in LPRs. Second, GPs were estimated in STAN
\parencite{R-cmdstanr} with a zero mean and a squared exponential covariance
function, following common practice. Third, GAMs with a single smooth term for
time were fitted using the MGCV package \parencite{R-mgcv_a}. Finally,
parametric differential equation models corresponding to the true
data-generating models were created and estimated using the Dynr package
\parencite{R-dynr}. While the same non-parametric models were used across all
conditions, the parametric models were tailored to each specific
data-generating process. A detailed description of each model fitting procedure
is provided in Appendix B.

To ensure reliable model fit and reasonable inferences, the fitting procedures
for each method were validated on pilot samples within each condition. After
this an initial run of the simulation was performed, which revealed
that the GAMs over- or linearly underfit some data sets and that the parametric
models overfit some data sets. To prevent this, the fitting procedure of
both methods was adjusted and the simulation rerun. Further, if any
models failed to converge during the simulation, the corresponding outcome
measures were excluded from the following analyses. After fitting each model to
the data, they were used to obtain point and interval estimates (i.e.,
95\% confidence and credible intervals) for the latent process at each time
point.

\subsection{Outcome measures}

To evaluate and compare the performance of the different analysis methods, we
focused on three outcome measures. The first two assessed each method's
accuracy in predicting the process values at or between the observed time
points. These predictive accuracy measures indicate how well each method
captures the underlying non-linear process. The third outcome measure evaluated
the accuracy of the uncertainty estimates provided by each method.
Specifically, whether the confidence or credible intervals produced
by each method correctly included the true state value the expected proportion
of times.

\subsubsection{Capturing the non-linear process}

To assess how effectively each method captured the non-linear process at the
observed time points, we calculated the mean squared error (MSE) between the
estimated and generated process values. Additionally,
to evaluate how well each method
interpolated the process between the observed time points, we computed the
generalized cross-validation (GCV; \textcite{golub_generalized_1979}) criterion
for each method and data set. The GCV is a more computationally efficient and
rotation-invariant version of the ordinary leave-one-out cross-validation
criterion, with a similar interpretation. The latter is calculated by removing
one data point, refitting the
model while keeping certain parameters fixed, predicting the left-out
observation, and calculating the squared
prediction error. By repeating this procedure for each data point and averaging
the squared errors, leave-one-out cross-validation provides an estimate of
how accurately the model
predicts unobserved values within the design range.

% Subsequently, a MANOVA was conducted to explain differences in the RMSE and
% GCV values between the simulation conditions and analysis methods. In order to
% also find evidence in favor of which factors likely do not affect the RMSE and
% GCV values, we conducted an exhaustive model search. Here the AIC and BIC
% based model weights were used as a criterion to identify which model is closest
% to the true data generating model. The AIC weights can directly
% be interpreted as the conditional model probabilities
% \parencite{wagenmakers_aic_2004}. Thus, if the true
% data generating model is among the tested models, the AIC weights indicate the
% probability that each tested model is the correct model given the data. This
% makes it possible, to quantify evidence both in favor and against the presence
% of any effect that could be included in the model. After establishing the most
% likely MANOVA model, we probed the included effects further by fitting
% univariate ANOVAs with the same predictors to the RMSE and the GCV values and
% followed up with Tukey's Honest Significant Difference test for the
% post-hoc comparisons.

\subsubsection{Uncertainty quantification}

To evaluate the uncertainty estimates provided by each method, we recorded
whether the true generated process was located within the confidence or
credible intervals at each time point.
Subsequently, the average
confidence interval coverage proportion for each method and data set was
obtained, by averaging over all time points.
Given that all confidence or credible intervals were set at a
95\% confidence level, the expected coverage proportion should ideally be
close to 95\%. Due to Monte Carlo error in the simulation, average coverage
proportions between 93\% and 97\% were also deemed acceptable.
Average coverage proportions above 97\% suggested overestimated standard
errors,
while those below 93\% indicated either a poor approximation of the underlying
process or an underestimation of the standard errors.

\subsection{Results}

\section{An Empirical Example}

In the following, the four analysis methods previously introduced were
applied to depression data from the Leuven clinical study. This study employed
experience sampling measures to study the dynamics of anhedonia in
individuals with major depressive disorder. This study was selected for its
heterogeneous sample, which includes participants with major depressive
disorder, borderline personality disorder, and healthy controls. This diversity
increases the likelihood of the data exhibiting a range of non-linear dynamics
and processes. Specifically, Houben et al. (date) found in their meta-analysis
that individuals with lower psychological well-being tend to experience greater
emotional variability, less emotional stability, and higher emotional inertia.
However, this finding did not replicate in an analysis of positive affect
within the Leuven clinical study (Hanega et al.). Additionally, emotional
inertia, the extent to which an emotional state carries over across
observations, has been shown to vary within individuals over time, which makes
it likely that the processes underlying this data are non-stationary.
% Fix this sentence
Combined with the previously outlined hypothesis
that emotion regulation may function as a self-regulating system, this data
is optimal for demonstrating the introduced methods..

To maintain consistency with how the methods were introduced and to avoid using
measurement models with multiple indicators, we analyzed momentary
depression, which was measured using a single item. This item was chosen over
affect measures because it displays sufficient variability, has a relatively
low proportion of participants with strong floor or ceiling effects, and is
measured on a broad response scale (0 to 100), making it ideal for illustrating
the introduced methods.

\subsection{Sample and data description}

The participants in the clinical sample of the Leuven clincial study were
screened by clinicians during the intake
in three Belgian psychiatric wards. Patients who met the DSM
criteria for mood disorders or borderline personality disorder during the
intake were eligible for enrollment, while those presenting with acute
psychosis, mania, addiction, or (neuro-)cognitive symptoms were excluded.
Following the screening, 90 patients enrolled in the study. Additionally, 44
control participants were matched to the clinical sample by gender and age,
resulting in a total sample size of 134.
Within the clinical sample, three patients withdrew during the baseline
assessment, two were excluded due to faulty devices, and seven were removed for
responding to less than half of the EMA measures. In the control sample, one
participant was excluded for responding to less than half of the EMA measures,
and three were removed for meeting the criteria for a current psychiatric
disorder. Consequently, the final published data set included 78 participants
in the clinical sample and 40 participants in the control sample.

During the study, all participants completed a baseline assessment, followed by
seven days of semi-random EMA assessments, with 10 equidistant assessments per
day. However, the starting date of the EMA measures varied between people.
During each assessment, participants responded to 27 questions covering
emotions, social expectancies, emotion regulation, context, and psychiatric
symptoms. This analysis focused on the item assessing momentary depressive mood
(i.e., "How depressed do you feel at the moment?") rated on a scale from 0 to
100. One additional participant was removed from the analysis for consistently
reporting a score of zero across all assessments, resulting in a final sample
of 117 participants. For a more thorough sample and data description, see
Hanega et al.

The published data set was obtained from the EMOTE database. The initial study
procedure was approved by the KU Leuven Social and Societal Ethics Committee
and the KU Leuven Medical Ethics Committee. This secondary data analysis was
approved by the Ethics Review Board of the Tilburg School of Social and
Behavioral Sciences.

\subsection{Analysis Plan}

\subsubsection{Exploratorty idiographic analysis}

First, the LPR, GP, and GAM methods were applied to explore the idiographic
latent processes underlying the data. Each method was applied separately to the
time-series of each participant, using the same specifications as in the
simulation study (Appendix B). However, for the LPR, only local cubic
polynomials were
considered to keep the interpretation of the bandwidth consistent across
participants.
Since all participants were assessed over seven days, but not during the same
period, the time-series for each participant was centered so that the first
measurement time point served as the zero point. The LPR bandwidth, GP
lengthscale, and GAM smoothing parameter were then analyzed to assess the
wigglyness of the idiographic processes. Additionally, the GCV values produced
by each method were evaluated to determine which method provided the most
accurate interpolations. Lastly, the mean squared error was calculated for each
method and data set to estimate the expected measurement error.

\subsubsection{Multilevel analysis}

After the idiographic analysis, a mixed effects GAM was fitted to the data.
This GAM employs sum-to-zero smooth interactions to create a model that
combines a single fixed non-linear process across participants with
individual-specific random non-linear deviations from the fixed effect. This
approach models both the average effect across individuals and individual
differences, similar to a linear mixed effects model. To preserve possible
time-of-day and day-of-week effects in the fixed smooth component across
participants, the time-series for all participants were aligned to the same
week while preserving the time and weekday of each assessment. Further, to
reduce computational complexity, a single smoothing parameter was applied to
all idiographic smooth functions.

\subsubsection{Parametric analysis}

Lastly, three parametric differential equation models were applied to explore
the data. The first model is a random-walk model, where the rate of change in
the latent depression state does not depend on its current value, such that
future values are only influenced by random perturbations in form of the
dynamic errors.
The second model is a continuous-time autoregressive model, where the
depression score reverts to an individual's mean at a rate linearly dependent
on the distance from this mean. Both the individual mean and autoregressive
effect were treated as random effects by incorporating them as time-invariant
state variables, similar to how random effects are included
in dynamic structural equation models.
The third model is the damped oscillator model discussed earlier. To adapt this
model to the data structure, a random mean was added to account for
between-person differences in the baseline level around which the depression
score oscillates. Random effects were also included for the spring constant
and damping coefficient to accommodate individual differences in oscillation
patterns. All three models included components for measurement and dynamic
error variance. Lastly, the model fit was compared between the models
using the AIC and BIC.

\subsection{Results}

\section{Discussion}

\section{Conclusion}

\printbibliography[]

\end{document}