% -----------------------------------------------------------------------------%
% Title:
% % Author: Jan Ian Failenschmid % Created Date: 13-03-2024
% %
% -----                                                                        %
% Last Modified: 30-01-2025                                                    %
% % Modified By: Jan Ian Failenschmid
% %
% %
% %
% -----                                                                        %
% Copyright (c) 2024 by Jan Ian Failenschmid
% % E-mail: J.I.Failenschmid@tilburguniveristy.edu
% %
% -----                                                                        %
% License: GNU General Public License v3.0 or later
% % License URL: https://www.gnu.org/licenses/gpl-3.0-standalone.html
% %
% -----------------------------------------------------------------------------%

\documentclass[man, floatsintext]{apa7}

% Dependencies
\usepackage{csquotes, amsmath, amssymb, mathptmx, enumitem, array, enumitem}
\usepackage{tabularx, makecell, setspace, courier, bm}
\usepackage[american]{babel}
\usepackage[figuresleft]{rotating}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa} \graphicspath{ {./figures} }
% Gaphics Path
\addbibresource{bibliography.bib} % Literature bibliography
\addbibresource{R_bib.bib} % R bibliography
\DeclareMathOperator*{\argmin}{argmin}
\setcounter{secnumdepth}{3}
\parindent=0pt
\newcolumntype{s}{>{\hsize=.6\hsize}X}
\renewcommand\theadfont{\normalsize\bfseries}
\usepackage{etoolbox}
\AtBeginEnvironment{tabularx}{\setlist[itemize, 1]{wide,
    leftmargin=*, itemsep=0pt, before=\vspace{-\dimexpr\baselineskip +2
      \partopsep}, after=\vspace{-\baselineskip}}}
% Title Page
\title{Modeling Non-Linear Psychological Processes: Reviewing and Evaluating
  Non-parametric Approaches and Their Applicability to Intensive
  Longitudinal Data}

\shorttitle{Modeling Non-Linear Psychological Processes}

\leftheader{Failenschmid}

\authorsnames{{Jan I. Failenschmid}, {Leonie V.D.E. Vogelsmeier}, {Joris
      Mulder}, {Joran Jongerling}}

\authorsaffiliations{{Tilburg University}}

\abstract{
  Psychological concepts are increasingly understood as complex dynamic systems
  that change over time. To study these complex systems, researchers
  are increasingly gathering intensive longitudinal data (ILD), revealing
  non-linear phenomena such as asymptotic growth, mean-level switching,
  and regulatory oscillations. However, psychological researchers currently
  lack advanced statistical methods that are flexible enough to capture these
  non-linear processes accurately, which hinders theory development. While
  methods such as local polynomial regression, Gaussian processes, and
  generalized additive models (GAMs) exist outside of psychology, they are
  rarely applied within the field because they have not yet been reviewed
  accessibly and evaluated within the context of ILD\@. To address this
  important gap, this article introduces these three methods for an applied
  psychological audience. We further conducted a simulation study, which
  demonstrates that all three methods infer non-linear processes that have
  been found in ILD more accurately than polynomial regression. Particularly,
  GAMs closely captured the underlying processes, performing almost as well as
  the data-generating parametric models. Finally, we illustrate how GAMs can be
  applied to explore idiographic processes and identify potential phenomena in
  ILD\@. This comprehensive analysis empowers psychological researchers to
  model non-linear processes accurately and select a method that aligns with
  their data and research goals.
}

\keywords{Non-linearity, intensive longitudinal data, local polynomial
  regression, Gaussian process, generalized additive model, non-parametric
  regression}

\authornote{ \addORCIDlink{Jan I. Failenschmid}{0009-0007-5106-7263}

  Correspondence concerning this article should be addressed to Jan I.
  Failenschmid, Tilburg School of Social and Behavioral Sciences: Department of
  Methodology and Statistics, Tilburg University, Warandelaan 2, 5037AB
  Tilburg, Netherlands. E-mail: J.I.Failenschmid@tilburguniversity.edu

  \noindent \textbf{Data and code availability statement:} \\
  \noindent  The simulated data and code that support the findings of this
  study are available in the online supplementary material. The data
  from the Leuven Clinical Study are available upon request from the
  \href{https://emotedatabase.com/datasets/3}{EMOTE database}.

  \noindent \textbf{Disclosure of artificial intelligence-generated content
    (AIGC) tools:} \\
  \noindent The use of AIGC tools was limited to improving spelling, grammar,
  and general editing.

  \noindent \textbf{Acknowledgements:}\\
  \noindent This project was supported by a Starter Grant. Additionally, J.M.
  was supported by an ERC Consolidator Grant (101087383).

  \newpage

  \noindent \textbf{Author contribution:}\\
  \noindent \textbf{Jan I. Failenschmid}: Writing - Original Draft Preparation,
  Software, Formal Analysis, Data Curation, Conceptualization, Investigation,
  Methodology, Visualization. \textbf{Leonie V.D.E. Vogelsmeier}: Writing -
  Original Draft Preparation, Writing - Review \& Editing, Supervision,
  Conceptualization, Funding Acquisition, Project Administration,
  Investigation, Methodology. \textbf{Joris Mulder}: Writing - Review \&
  Editing, Supervision, Conceptualization, Project Administration.
  \textbf{Joran Jongerling}: Writing - Original Draft Preparation, Writing -
  Review \& Editing, Supervision, Conceptualization, Funding Acquisition,
  Project Administration, Investigation, Methodology, Formal Analysis.
}

\begin{document}

\maketitle

Psychological constructs are increasingly understood as components of complex
dynamic systems \parencite{nesselroade_studying_2004, wang_investigating_2012}.
This perspective emphasizes that these constructs fluctuate over time and
within individuals. To study these variations and the underlying processes,
researchers are increasingly collecting intensive longitudinal data (ILD) using
ecological momentary assessment (EMA), experience sampling, or similar methods
\parencite{fritz_so_2023}. In these studies, one or more individuals are
assessed at a high frequency (multiple times per day) using brief
questionnaires or passive measurement devices. These rich data allow
researchers to examine complex temporal variations in the underlying
psychological variables within an ecologically valid context and to explain
them through (between-person differences) in within-person processes.

Through ILD studies, many non-linear psychological phenomena and processes have
been discovered in recent years, \textcolor{blue}{which either show
  monotonically increasing or decreasing non-linear trajectories or
  non-monotonic
  gradual or even abrupt change}. Clear examples of \textcolor{blue}{ gradual
  monotonically increasing processes} are the learning and growth curves
observed
in intellectual and cognitive development \parencite{kunnen_dynamic_2012,
  mcardle_comparative_2002}. In these cases, an individual's latent ability
increases over time, following an intricate non-linear trajectory from a
(person-specific) starting point towards a (person-specific) asymptote, which
reflects the individual's maximum ability. Additional examples of asymptotic
growth over shorter time spans that are typically studied with ILD include
motor skill development \parencite{newell_time_2001} and second language
acquisition \parencite{de_bot_dynamic_2007}. Figure~\ref{fig:examplar_npn}
shows common \textcolor{blue}{deterministic} model choices for these kinds of
processes in the form of an exponential growth function (a) and a logistic
growth function (b). \textcolor{blue}{It is noteworthy that this exponential
  growth function has been modeled as a deterministic Ornstein-Uhlenbeck
  process, which is the continuous-time equivalent to the common lag-one
  autoregressive model. The (person-specific) mean and slope of the
  autoregressive model therefore correspond to the asymptote and the rate of
  change of the exponential growth model.}

\begin{figure}[!t]
  \caption{Examples of non-linear processes demonstrated to occur in
    psychological time series}
  \fitfigure{exemplar_no_process_noise.png}
  \figurenote{This figure shows four demonstrated psychological non-linear
    processes. \textcolor{blue}{These processes were modelled using common
      deterministic or ordinary differential equation models.} Panels (a) and
    (b)
    show exponential and logistic growth curves,
    respectively. Panel (c) shows a cusp catastrophe model. Lastly, panel (d)
    shows a damped oscillator.}
  \label{fig:examplar_npn}
\end{figure}

\textcolor{blue}{A common example of non-monotonic abrupt change}
is switching between seemingly distinct
states that differ, for instance, in their means. This occurs, for example,
during the sudden perception of cognitive flow, where individuals abruptly
switch from a "normal" state to a flow state and back
\parencite{ceja_suddenly_2012}. Another example of this is alcohol use relapse,
where patients suddenly switch from an abstinent state to a relapsed state
\parencite{witkiewitz_modeling_2007}. This sudden switching behavior has been
modelled using a cusp catastrophe model
\parencite{van_der_maas_sudden_2003,chow_cusp_2015}. This dynamic model has
been exemplified in Figure~\ref{fig:examplar_npn}~(c).

As a final example, \textcolor{blue}{we consider non-monotonic gradual change.
  This can be found, for example,} in (self-) regulatory systems, which
maintain a desired state by counteracting external perturbations. In these
systems, the degree of regulation often depends on the distance between the
current and the desired states. The common lag one autoregressive model
describes such a system in which the regulation strength depends linearly on
this distance, \textcolor{blue}{resulting in an exponential decay towards the
  baseline}. However, this relationship may also be non-linear, such that the
degree of regulation changes disproportionately with larger distances. One
example of such a possible (self-) regulatory system is emotion regulation,
which has been modelled using a damped oscillator model
\parencite{chow_emotion_2005}. This model is exemplified in
Figure~\ref{fig:examplar_npn}~(d).

Although initial evidence for non-linearity in psychological research exists,
theories about the nature and form of non-linear psychological processes remain
scarce \parencite{tan_time-varying_2011}. Frequently, psychological theories
are too general to result in specific hypotheses
\parencite{oberauer_addressing_2019}, such as, for example, about the specific
form of non-linear dynamics. ILD studies can potentially help to refine such
theories by providing a nuanced understanding of how psychological variables
interact over time. These refined theories could, for instance, take the form
of parametric dynamic models, such as differential equation
\parencite{cooper_dynamical_2012} or state-space models
\parencite{durbin_time_2012}, that describe how a given process changes over
time. However, in order to develop these more specific theories (about the form
of non-linear change), it is first necessary to empirically uncover, observe,
and study phenomena such as the mentioned state switching or regulatory
oscillations in ILD, and determine if they generalize beyond individual data
sets and contexts. Formal theories about the underlying process should then be
able to explain these phenomena and different candidate theories can be
compared based on their success in doing so \parencite{borsboom_theory_2021}.
While the study of non-linear phenomena in ILD is receiving increasingly more
attention in psychology and different statistical techniques are developed to
explore these phenomena \parencite{cui_unlocking_2023,humberg_estimating_2024},
researchers are currently still limited in their ability to infer non-linear
phenomena from ILD\@. One reason for this is \textcolor{blue}{that} advanced
statistical methods that are flexible enough to adequately capture and explore
these processes \textcolor{blue}{are currently not well established in
  psychological research}, which hinders the development and evaluation of
guiding theories.

Due to adequate statistical methods \textcolor{blue}{not being commonly
  available}, non-linear trends in
psychology are often addressed through polynomial or piecewise spline
regression. Polynomial regression \parencite{jebb_time_2015} uses higher-order
terms (e.g., squared or cubed time, in addition to a linear effect of time) as
predictors in a standard multiple linear regression model. While effective for
relatively simple non-linear relationships, particularly those that can be
represented as polynomials, this method has significant limitations and likely
leads to invalid results when applied to more complex processes, such as mean
switching or (self-) regulatory systems (e.g.,
Figure~\ref{fig:examplar_npn}~c~\&~d).

\textcolor{blue}{
  When fitting models with many degrees of freedom, such as a polynomial
  regression with many higher-order terms, the risk of underfitting or
  overfitting arises quickly. Underfitting occurs when the model is too simple
  or inflexible to accurately capture the underlying process, whereas
  overfitting occurs when the model is overly flexible and adapts to random
  noise in the data, resulting in poor generalization and unreliable
  predictions for new samples. For complex processes, polynomial approximations
  often require many higher-order terms to accurately capture the intricate
  trajectory. However, the large number of highly correlated parameters in
  these models can lead to model instability, which can cause convergence
  issues during estimation. This instability may force researchers to use
  simpler models, even though these might not be able to capture more complex
  processes accurately. Even when
  the polynomial degree is optimal, the non-locality of polynomials can result
  in overfitting at the boundaries of the process while underfitting the
  central regions. This may also produce non-sensical predictions, such as
  interpolating scores outside the valid range of the scale
  \textcite{magee_nonlocal_1998, boyd_divergence_2009, harrell_general_2001,
    jianan_case_2023}.
}

An alternative approach is piecewise spline regression, which constructs a
(complex) non-linear trend by joining multiple simple piecewise functions
together at specific points, called knots (e.g., connecting multiple cubic
functions end-to-end into an overarching growth curve;
\textcite{tsay_nonlinear_2019}). \textcolor{blue}{This alleviates some of the
  previously described issues by creating localized models. These models have
  less correlated parameters and are therefore less prone to model instability.
  However, piecewise spline regression still requires researchers to carefully
  select the optimal piecewise functions and knot locations, which may be done
  manually or in a data-driven manner}. This can be problematic in practice
because, as mentioned, precise guiding theories about the functional form of
most psychological processes are lacking \parencite{tan_time-varying_2011}.
This absence of clear guidance can quickly lead to misspecified models and
invalid results.

These limitations in the currently available methods underscore the need for
more sophisticated and data-driven statistical methods to study and explore
non-linear processes. Various such advanced statistical methods like local
polynomial regression \parencite{fan_local_2018}, Gaussian processes
\parencite{rasmussen_gaussian_2006}, and generalized additive models
\parencite{wood_generalized_2006} are available outside of psychology. However,
these methods have rarely been applied in psychology because they have not been
reviewed for an applied audience, nor have their assumptions and inference
possibilities been evaluated in the context of ILD\@. As a result, it is
difficult for psychological researchers to select the most suitable method for
a specific context. Moreover, the ideal statistical method may depend on the
characteristics of the underlying non-linear process (which, as yet,  are
generally unknown). Especially, since the types of non-linear processes for
which the different methods were developed match the types of processes
occurring in psychological research (e.g., the processes depicted in Figure 1)
to varying degrees, particularly with regard to the assumed smoothness of the
non-linear process. This means that the method most suitable for the relatively
smooth process in Figure 1a is not necessarily also the most suitable method
for rougher changes.

To address this important gap, this article reviews three advanced non-linear
analysis methods, local polynomial regression, Gaussian processes, and
generalized additive modelsm, and evaluates their applicability to typical ILD
scenarios (Section~\ref{method_introduction}). \textcolor{blue}{ The methods
  reviewed in this article are various semi- and non-parametric regression
  techniques that were chosen for their ability to model non-linear processes
  while accommodating different levels of prior knowledge. Our review
  specifically focuses on methods that provide a clear and transparent
  mechanism
  for inferring non-linear processes and for preventing over- and underfitting
  the
  data. Additionally, we limited our selection to methods that are implemented
  in
  the open-source software R \parencite{R-base}, making them readily accessible
  for research applications. Lastly, we prioritized methods that are promising
  for application to psychological ILD, particularly in terms of sample size
  and
  measurement frequency requirements.} We additionally compare how well each
method recovers different non-linear processes under common ILD conditions in a
simulation study (Section~\ref{simulation}). Lastly, we demonstrate how the
best-performing method can be applied to analyze an existing data set
(Section~\ref{empirical_example}). Note that, to introduce these methods
accessibly and apply them under conditions where software implementations are
available, this article focuses on the univariate single-subject design.

The methods reviewed in this article are various semi- and non-parametric
regression techniques, that were chosen for their ability to model non-linear
processes
while accommodating different levels of prior knowledge. Our review
specifically focuses on methods that provide a clear and transparent mechanism
for inferring latent processes and for preventing
over- and underfitting the data. Additionally, we limited our
selection to methods that are implemented in the open-source software R
\parencite{R-base}, making them readily accessible for research applications.
Lastly, we prioritized methods that are promising for application to
psychological ILD, particularly in terms of sample
size and measurement frequency requirements.

\section{Non-linear analysis methods}\label{method_introduction}

In the following paragraphs, the three semi- and non-parametric regression
techniques will be introduced: local polynomial regression, Gaussian processes,
and generalized additive models. \textcolor{blue}{
  Due to their non-parametric nature, these methods allow researchers to
  explore
  the trajectories of psychological processes without requiring prior
  assumptions
  about their parametric functional form, similar to how polynomial regression
  is
  currently often applied. This flexibility is particularly valuable for theory
  development, as formal theories about psychological processes are often
  represented as parametric models. By providing accurate estimates of the
  underlying processes, these methods can help researchers identify functional
  forms that are strong candidates for parametric models by highlighting key
  features in the data that these models should capture.
  Additionally, semi-parametric methods can combine incomplete parametric
  models with flexible non-parametric components. This ability to test partial
  theories while accurately capturing the underlying processes makes these
  methods especially useful for iterative theory development by, for example,
  identifying areas of misfit in parametric models.}

\subsection{Local polynomial regression}

\subsubsection{\textcolor{blue}{Statistical theory}}

The first technique \textcolor{blue}{we are reviewing} is called local
polynomial regression (LPR). Similarly to regular polynomial regression, LPR
approximates the process using polynomial basis functions (e.g., squared or
cubed time). However, instead of using one large polynomial function to
approximate the entire process, LPR estimates smaller, local polynomials at
every point in time. These local polynomials are then combined into a single
non-linear function over the entire set of observations
\parencite{fan_adaptive_1995, ruppert_multivariate_1994, fan_local_2018}.
\textcolor{blue}{This technique shares the same principle as and is a
  generalization of the rolling average for timeseries smoothing. A rolling
  average assumes that observations that are close in time are likely noisy
  indicators of similar process values. Therefore, the value of the process can
  be estimated at any point in time by averaging out the measurement error of
  data points in close proximity. LPR generalizes this principle by using more
  flexible polynomial regressions instead of an average to approximate the
  process pointwise.
  Figure~\ref{fig:locpol_dem}~(a) shows the estimated LPR (solid black line)
  for the damped oscillator example process (dotted black line) introduced in
  Figure~\ref{fig:examplar_npn}~(d). Three of the one 100 local cubic
  polynomials that were used to estimate this process are shown in red.
  Figure~\ref{fig:locpol_dem}~(b) zooms in to
  illustrate how each local cubic regression estimates the process value at a
  single time point, which are subsequently connected into an overall estimate
  of the process.}

\begin{sidewaysfigure*}[htbp]
  \caption{Demonstration of a local polynomial regression}
  \fitfigure{locpol_demonstration.png}
  \figurenote{This figure shows how LPR (solid black) estimates the
    underlying
    process (dotted black). \textcolor{blue}{
      Panel (a) shows three of the 100 local cubic regressions used to
      construct
      the overall estimate of the process (red). Panel (b) provides a zoomed-in
      view of a specific section of panel (a), illustrating that the estimated
      process is composed of the intercepts of the three local cubic
      regressions.
      Panel (c) depicts the Epanechnikov kernel that was used in the LPR
      depicted
      in panel (a). Panels (d) to (f) illustrate the impact of varying the
      bandwidth of a local cubic regression. Panel (d) highlights overfitting
      resulting from a bandwidth that is too narrow, panel (e) shows an
      optimized
      bandwidth, and panel (f) depicts underfitting caused by a bandwidth that
      is
      too wide. Lastly, panels (g) to (i) display optimized local linear (g),
      quadratic (h), and quartic (i) regressions, showing the effect of varying
      the polynomial degree.}
  }
  \label{fig:locpol_dem}
\end{sidewaysfigure*}

\textcolor{blue}{
  Specifically, to predict the value of the univariate process $f$ at an
  arbitrary time point $t^*$ based on the set of observations $y_t$ at the time
  points $t$ the LPR starts by cenetering the data around the time point $t^*$
  (by shifting the data along the time axis so that the chosen time point is at
  zero). Afterwards, a low order polynomial regression is fit to the data
  around $t^*$.
}

\begin{align}
  y_t             & = f(t) + \epsilon_t \\
  \textbf{X}(t^*) & =
  \begin{bmatrix}
    1      & (t_1 - t^*)^1 & \dots  & (t_1 - t^*)^p \\
    \vdots & \vdots        & \ddots & \vdots        \\
    1      & (t_n - t^*)^1 & \dots  & (t_n - t^*)^p
  \end{bmatrix}
  \label{eq:lpr_equations_mod_mat}
\end{align}

\noindent\textcolor{blue}{
  This polynomial regression uses the predictor matrix $\textbf{X}$, in which
  each column corresponds to a polynomial transformations of time centered
  around $t^*$ (i.e., the first column is filled with ones, the second column
  contains the centered time points, the third column contains the centered
  time points squared, and so on up to the chosen degree $p$ of the
  polynomial). The degree $p$ of the polynomial regression reflects an
  assumption about how smooth the underlying process is. Specifically, for a
  first-degree LPR, the process should not exhibit any corners,
  discontinuities, or vertical sections. This ensures that the process's rate
  of change (i.e., first derivative), approximated by the first-order
  polynomial term, is well-behaved. Higher-order local polynomials require this
  smoothness for increasingly complex rates of change. For instance, a
  second-degree LPR requires that the rate of change itself is smooth, which
  ensures that its rate of change is well-behaved. This property should hold
  for all $p$ rates of change of a process when using an LPR with $p$
  degrees\footnote{Since every polynomial term in the LPR estimates one
    derivative term in the corresponding Taylor series approximation
    \parencite{avery_literature_nodate}.}.
}

\textcolor{blue}{
  Since this polynomial approximation is more accurate for data points closer
  in time, an additional weighting function is introduced in the polynomial
  regression, which assigns deminishing weights to points that are further away
  from $t^*$.
}

\begin{align}
  \textbf{W}(t^*) & =
  \begin{bmatrix}
    w_{1, 1}(t^*) &        &               \\
                  & \ddots &               \\
                  &        & w_{n, n}(t^*)
  \end{bmatrix}               \\
  w(t^*)_{i, i}   & = k_{LPR}(\frac{|t_i - t^*|}{h})
  \label{eq:lpr_equations_weights}
\end{align}

\noindent\textcolor{blue}{ The exact weights for each datum are determined by a
  kernel function $k_{LPR}$. Common choices for this kernel function are the
  Gaussian and Epanechnikov (Figure~\ref{fig:locpol_dem}~(c)) kernels. Both
  kernels assign higher weights to data points in closer proximity to the point
  of interest $t^*$, however, while the Gaussian kernel assigns small weights
  to all distant data points, the Epanechnikov kernel assigns zero weights
  beyond a certain distance. The kernel function additionally includes a
  bandwidth parameter $h$, which controls the width of the kernel and regulates
  the rate at which the weights decrease with increasing distance. }

\textcolor{blue}{
  Finally, the value that the LPR predicts for $t^*$ is given
  by the intercept of the fitted locally weighted polynomial around $t^*$.
  The coefficient vector }$\hat{\boldsymbol{\beta}}$\textcolor{blue}{, whose
  first element is the
  intercept, for such a weighted polynomial regression can be obtained through
  the normal equation in Equation~\ref{eq:lpr_equations_last}.
}

\begin{align}
  \hat{\beta}(t^*) & =
  (\textbf{X}(t^*)^T\textbf{W}(t^*)\textbf{X}(t^*))^{-1}
  \textbf{X}(t^*)^T\textbf{W}(t^*)\textbf{y}
  \label{eq:lpr_equations_last}                \\
  \hat{f}(t^*)     & = \hat{\bm{\beta}}_1(t^*)
\end{align}

This procedure is repeated for all
time points of interest and the estimated LPR values are subsequently connected
into an overall estimate of the nonlinear process.
As it is theoretically possible to repeat this process at infinitely many
time points, LPR is a non-parametric technique.

\subsubsection{\textcolor{blue}{Practical implementation}}

\textcolor{blue}{
  When fitting an LPR, researchers need to make three decisions concerning the
  kernel function, the bandwidth of the kernel, and the degree of the local
  polynomials. Regarding the kernel functions, common choices include the
  Gaussian, the Epanechnikov, or the uniform kernel, which assigns equal
  weights to all data points up to a certain distance. Because the Epanechnikov
  kernel has been shown to be optimal in many situations
  \parencite{fan_local_1997}, it constitutes a good default choice if there are
  no strong preferences for other kernels. In practice, different kernel
  functions rarely result in strongly differing estimates. Nevertheless, it is
  advisable to conduct sensitivity analyses to assess whether any conclusions
  depend (strongly) on the chosen kernel.
}

\textcolor{blue}{
  The bandwidth parameter effectively determines the wiggliness of the
  estimated process. A bandwidth that is too small results in an LPR that is
  too flexible and overfits the data (Figure~\ref{fig:locpol_dem}~(d)).
  Conversely, a bandwidth that is too large results in an LPR that is not
  flexible enough and underfits the data (Figure~\ref{fig:locpol_dem}~(f)).
  Therefore, it is critical to the performance of the LPR to find an optimal
  bandwidth that strikes a balance between being flexible enough to adequately
  capture the underlying process and not overfitting the data
  (Figure~\ref{fig:locpol_dem}~(e)).} Several methods are available to find
the
optimal bandwidth by optimizing a data-dependent criterion function, such as
the cross-validation error or the mean integrated squared error
\parencite{kohler_review_2014, debruyne_model_2008}. This optimization
criterion may be selected based on specific research objectives. For instance,
optimizing the cross-validation error may be most attractive if the primary
research interest is out of sample prediction. However, most standard software
packages offer automated default procedures to find the optimal bandwidth,
which is especially attractive for researchers new to LPR\@. Note that the fact
that the LPR uses a single value for the bandwidth parameter implies that the
method assumes that the underlying process has constant wiggliness (with
respect to the degree of the local polynomials). This
constant wiggliness assumption can be relaxed by using a time-varying bandwidth
\parencite{fan_data-driven_1995} or a time-varying polynomial degree
\parencite{fan_adaptive_1995}, but these extensions are beyond the scope of
this paper.

\textcolor{blue}{
  Lastly, the degree of the local polynomial must be chosen. Beyond reflecting
  an assumption about the smoothness of the process, higher-degree polynomials
  offer greater flexibility to approximate the process using a single
  bandwidth. This can be seen in Figure~\ref{fig:locpol_dem}, in which the
  local linear (g) and local quadratic (f) regressions are more rigid in
  approximating the process than the local cubic (a) or quartic (i)
  regressions, even though all models use an optimized bandwidth. It is
  important to note that unless the underlying process follows a polynomial
  trajectory of at most the same degree as the LPR, the approximation with
  local polynomials will introduce some bias. For example, while a process with
  a quadratic trajectory can be accurately inferred at any point by a local
  quadratic regression (or any higher-order LPR), a process following an
  exponential trajectory cannot be perfectly captured by any LPR with a finite
  degree. Instead, there will be a small bias in the estimate, which decreases
  with higher polynomial degrees. However, this bias is typically negligible,
  and methods are available to correct for it. This bias also affects the
  standard errors and confidence intervals estimated by the LPR\@. In standard
  practice, these confidence intervals are estimated around a biased point
  estimate, which can lead to non-nominal coverage probabilities. While
  higher-degree LPRs reduce bias, they also increase the estimtator variance
  when transitioning from an odd to an even degree, leading to a bias-variance
  tradeoff \parencite{ruppert_multivariate_1994}. As a result, the polynomial
  degree is typically chosen to be low and odd, with local linear and cubic
  regressions being the most commonly used.
}

\textcolor{blue}{
An implementation of LPR can be found in the nprobust R package
\parencite{R-nprobust}. The LPR in Figure~\ref{fig:examplar_npn}~(a) can be fit
using: {\fontsize{10}{12}\selectfont\texttt{lprobust(y, time, eval = time, p =
  3, kernel = 'epa', bwselect = 'imse-dpi', bwcheck = 0)}}. Here we set the
polynomial degree to three, chose an Epanechnikov kernel, and specify that the
bandwidth should be optimized by minimizing the integrated mean squared error.
The LPR implemented in nprobust also automatically corrects for the possible
bias introduced by the finite polynomial degree. The last argument is used to
suppress an automated correction of the bandwidth towards the beginning and end
of the process. This code returns a bias-corrected estimate of the process, as
well as robust standard errors and the optimized bandwidth parameter, which can
be interpreted as an estimate of the wiggliness of the process.
}

\subsection{Gaussian process regression}

\subsubsection{\textcolor{blue}{Statistical theory}}

The second non-parametric technique is Gaussian process (GP) regression, a
Bayesian approach that directly defines a probability distribution over an
entire family of non-linear functions, which is flexible enough to capture many
complex processes effectively \parencite{rasmussen_gaussian_2006,
  betancourt_robust_2020, roberts_gaussian_2013}. Unlike regular probability
distributions (e.g., normal distributions) that specify the likelihood of
single values, GPs determine how likely entire (non-linear) functions are. A GP
is defined indirectly, such that if the functions it describes are evaluated at
any finite set of time points, the resulting sample of function values will
follow a multivariate normal distribution.

In a Bayesian framework, one can use
a GP to define a prior distribution for the latent process as $f \sim GP$.
Following Bayes' theorem, this prior is then combined with an appropriate
likelihood for the observed data to obtain a posterior distribution for the
latent process given the observed data:

\begin{equation}
  P(f \, | \, \textbf{y})  \propto P(\textbf{y} \, | \, f) P(f)
\end{equation}

\noindent This posterior distribution represents an updated belief about which
functions describe the latent process well, making it possible to draw
inferences about the process \parencite{rasmussen_gaussian_2006}.
\textcolor{blue}{
  Figure~\ref{fig:gp_dem}~(a) shows a sample of functions (in red) drawn from a
  GP prior. This prior was combined with a Gaussian likelihood to obtain a
  sample
  from the corresponding posterior distribution of the latent process, shown in
  Figure~\ref{fig:gp_dem}~(b). This sample of functions from the posterior GP
  can
  be used to obtain pointwise estimates and credible intervals for the
  underlying
  process.
}

\begin{sidewaysfigure*}[htbp]
  \caption{Demonstration of a Gaussian process regression}
  \fitfigure{gp_demonstration.png}
  \figurenote{This figure shows how GP regression (solid black) estimates the
    underlying process (dotted black). \textcolor{blue}{ Panel (a) shows a
      sample from a GP prior with a squared-exponential kernel and halfnormal
      hyperparameter priors. Panel (b) shows a sample from the corresponding GP
      posterior distribution. Panel (c) depicts the squared-exponential kernel
      that was used in panel (A) and (B). Panels (d) and (e) illustrate the
      impact of fixing the marginal variance of a GP prior to values that are
      too
      small or too large resepectively. Panel (f) depicts a Matérn 1/2 kernel
      that was used in the GP regression in panel (i). Lastly, panels (g) and
      (h)
      illustrate the effect of fixing the lengthscale of the GP prior to values
      that are too small or too large respectively.}
  }
  \label{fig:gp_dem}
\end{sidewaysfigure*}

\textcolor{blue}{
  A GP distribution is defined by a mean function $m(t)$ and a covariance
  function $cov(t, \, t)$. These functions are continuous extensions of the
  mean vector and covariance matrix of a multivariate normal distribution.
  Together, they define the mean and covariance of the values that the
  functions described by the GP assume at any finite set of time points.
}

\textcolor{blue}{
  In most applications, the mean function is set to zero when no specific prior
  knowledge is available. This results in a GP prior without an average trend,
  as illustrated in Figure~\ref{fig:gp_dem}~(a). However, this does not
  restrict the posterior mean to zero, as shown in Figure~\ref{fig:gp_dem}~(b),
  where the posterior mean closely follows the underlying process. } Instead,
it indicates a lack of prior information about its deviations from zero.
However, if there is prior information available about the trend of the
non-linear process (such as in a developmental context, where a learning
curve could be expected to have a general upwards trend) it would be possible
to include this in the GP by adding, for example, a linear or quadratic mean
function.

The covariance function is typically based on a kernel function, similar to the
kernel used in the LPR\@. The types of processes that can be captured by a GP
regression primarily depend on the chosen form of the covariance kernel. This
is because the covariance kernel largely dictates the behavior of the functions
described by the GP prior\@. \textcolor{blue}{The dot-product kernel
  (Equation~\ref{eq:dot_prod_kernel}), for example, generates a Gaussian
  process
  that describes linear functions and is equivalent to Bayesian linear
  regression. In this case, the variances $\sigma_\alpha^2$ and
  $\sigma_\beta^2$
  are represent the prior variances of the intercepts and slopes of the linear
  functions defined by the GP\@.}

\begin{equation}\label{eq:dot_prod_kernel}
  cov(t_i, t_j) = k_{GP}(t_i, t_j) = \sigma_\alpha^2 + \sigma_\beta^2 t_i t_j
\end{equation}

The default covariance kernel in most standard software is the
squared exponential (Figure~\ref{fig:gp_dem}~(c)), which produces a
non-linear Gaussian process that is covariance stationary (such that the
described covariances between the function values depend only on their
distance) and very smooth. \textcolor{blue}{This kernel was used for the GPs
  in Figure~\ref{fig:gp_dem}~(a) and (b). } Using the squared exponential
covariance kernel imposes a stricter smoothness assumption on the underlying
process than the smoothness assumption introduced for LPR\@. However, many
other covariance kernels are available, each resulting in GPs with different
behaviors and assumptions about the underlying process. A notable example is
the Matérn class of kernels, which relaxes the strict smoothness assumption
made by the squared exponential kernel. This makes it possible to model
rougher processes, as illustrated in (Figure~\ref{fig:gp_dem}~(i)).
\textcolor{blue}{The Matérn 1/2 kernel (Figure~\ref{fig:gp_dem}~(f)) may be
  particularly relevant for psychological applications, as it describes the
  covariance structure of a continuous-time autoregressive process.} We will,
however, focus on the squared exponential kernel, which is the default in most
standard software.

As was the case with the LPR, the kernels (and thus the covariance functions)
used in a GP have parameters called hyperparameters, which determine the scale
and variability of the estimated non-linear process. Most common covariance
kernels (such as the introduced squared exponential or the Matérn class
kernels) build on a characteristic lengthscale $\rho$ and a marginal standard
deviation parameter $\alpha$. The characteristic lengthscale effectively
determines the wiggliness of the estimated process by quantifying how quickly
the covariance decreases with increasing distances between time points. Such
that a smaller lengthscale corresponds to a wigglier process.
\textcolor{blue}{Since GPs typically use a single lengthscale for the entire
  process, they assume a constant level of wiggliness throughout
  the underlying process.}
The marginal standard deviation describes the spread of the functions described
by the GP at any point in time.

\subsubsection{\textcolor{blue}{Practical implementation}}

\textcolor{blue}{
  When fitting a GP regression, researchers must first select a mean function
  and covariance kernel. In the absence of prior knowledge about systematic
  deviations from zero, the mean function is typically set to zero. However,
  other mean functions can be chosen to incorporate specific domain knowledge.
  The squared exponential kernel is the default choice in most standard
  software due to its ability to produce smooth estimates. If less smooth
  estimates are preferred, the Matérn class of kernels may offer suitable
  alternatives. The choice of a specific kernel can be informed by prior
  theory, specific smoothness requirements, or in a data-driven manner by
  model comparison methods such as Bayes factors or cross-validation.
}

\textcolor{blue}{
  The hyperparameters of the GP could, for example, be estimated by optimizing
  either a cross-validation criterion or the model's marginal likelihood, in
  order to prevent over- or underfitting the data. Figure~\ref{fig:gp_dem}~(d)
  shows a GP with a marginal variance that is too small, making the GP prior
  too narrow and unable to adequately capture the process, which leads to
  underfitting. However, a marginal variance that is too large does not result
  in overfitting to the same degree, as shown in Figure~\ref{fig:gp_dem}~(e).
  Since the lengthscale of the GP controls its wiggliness, a lengthscale that
  is too small leads to overfitting (Figure~\ref{fig:gp_dem}~(g)), whereas a
  lengthscale that is too large results in underfitting
  (Figure~\ref{fig:gp_dem}~(h)). While the hyperparameters of a GP can be
  optimized in a similar way to those of the other methods presented in this
  paper, we recommend using a full Bayesian approach that assigns prior
  distributions to the hyperparameters. In cases where no prior information is
  available about the wiggliness or spread of the process, these priors can be
  vague to assume that all parameter values are equally likely a priori. In
  practice, it is often helpful to standardize the predictor and outcome
  variables to ensure that the hyperparameter priors are scaled appropriately.
  An advantage of this full Bayesian approach is that it provides uncertainty
  estimates, not only for the process itself, but also for the lengthscale and
  marginal variance.
}

\textcolor{blue}{
  The brms package \parencite{R-brms} provides an accessible implementation of
  Gaussian process regression using the model syntax
    {\fontsize{10}{12}\selectfont\texttt{brm(y $\sim$ gp(time))}}. This code
  automatically uses a squared-exponential kernel (which is currently the only
  kernel supported by brms) and assigns vague priors to the hyperparameters.
  Specific informative hyperparameter priors can be added to the
    {\fontsize{10}{12}\selectfont\texttt{brm}} function using the optional
    {\fontsize{10}{12}\selectfont\texttt{prior}} argument. The GP prior used in
  Figure~\ref{fig:gp_dem}~(a) assigns half-normal priors with standard
  deviations of one to the lengthscale and two to the marginal variance and
  standardizes the predictor and outcome variables. The brms model syntax also
  makes it straightforward to combine multiple GPs in a model or to incorporate
  parametric components, such as a linear slope. After fitting a GP,
  researchers
  obtain posterior samples for the underlying process and any hyperparameters
  included in the model. The posterior distribution for the lengthscale, in
  particular, can be understood as an estimate of the wiggliness of the
  process.
  These posterior samples can be used to construct point estimates and credible
  intervals for the process and the hyperparameters.
}

\subsection{Generalized additive models}

\subsubsection{\textcolor{blue}{Statistical theory}}

Generalized additive models (GAMs) are a class of semi-parametric models that
can be seen as an extension of regression models that \textcolor{blue}{does}
not use variables as predictors of an outcome but so-called smooth terms:

\begin{equation}
  Y_t = \beta_0 + \sum_{i = 1}^{I} \beta_i(t_i),
\end{equation}

\noindent where each smooth term $\beta_i$ reflects a non-linear function of a
predictor (e.g., time). A GAM combines these different smooth terms into an
overall estimate of a non-linear process \parencite{wood_generalized_2006,
  wood_inference_2020, hastie_generalized_1999}. The smooth terms $\beta_i$ are
inferred from the data through a method called smoothing splines. These
smoothing splines are very similar to the piecewise splines introduced in the
introduction in that the smoothing splines also take the form of a basis
function regression. \textcolor{blue}{
  This basis function regression
} does not use the raw predictor values directly. Instead, it
combines multiple predefined functions (called basis functions) of the
predictor $R_{ki}(t_i)$, such as polynomial basis functions of different
degrees, along with their weights $\alpha_{ki}$ in a regression model:

\begin{equation}
  \beta_i(t_i) = \sum^K_{k = 1} \alpha_{ki} R_{ki}(t_i)
\end{equation}

\noindent Generally, using more basis functions (and thus coefficients) results
in a smoothing spline (and thus smooth term) that is more flexible, which means
that it can fit the data closer during the estimation. Smoothing splines rely
on using enough basis functions \textcolor{blue}{to be able} to overfit the
data, which ensures that they are flexible enough to capture the underlying
process accurately.

In summary, GAMs use a sum of smooth terms to model the
outcome variable, where each smooth term is estimated using a smoothing spline.
These smoothing splines build on basis functions such that a greater amount of
basis functions yields a more flexible smoothing spline. \textcolor{blue}{
  Figure~\ref{fig:gam_dem}~(a) shows a GAM composed of a single smooth term
  that was fit to the example process. The ten weighted basis functions that
  constitute the smoothing spline are shown in red.
  Figure~\ref{fig:gam_dem}~(b)
  shows the unweighted basis functions as direct transformations of the
  predictor variable (i.e., time) before they are combined in the basis
  function
  regression.} While GAMs can combine multiple smooth terms, in this paper, we
will focus on
the simplest GAMs consisting of a single smooth term.

\begin{figure}[!t]
  \caption{Demonstration of the construction of a GAM}
  \fitfigure{gam_demonstration.png}
  \figurenote{This figure shows how generalized additive models (solid black)
    estimate the underlying process (dotted black).
    \textcolor{blue}{Panel (a) shows a fitted
      GAM with a single smooth term for time. The ten weighted thin-plate
      spline basis functions (red) sum to produce an estimated of the
      underlying. The unweeighted basis functions are depicted in panel (b).
      Panel (c) and (d) illustrate the effect of fitting a GAM with too few
      or too many basis functions resepectively. Similarly, panel (e) and (f)
      illustrate GAMs with a smoothing penalty that is too small or too large,
      leading to over- or underfitting respectively.}
  }
  \label{fig:gam_dem}
\end{figure}

\textcolor{blue}{ Smoothing splines can be constructed using different families
  of basis functions, called spline bases. } The most common
\textcolor{blue}{basis is} thin-plate splines, which are a generalization of
cubic regression splines \parencite{wood_generalized_2006}. In contrast to the
piecewise
splines introduced earlier, thin-plate splines entirely avoid knots. Instead,
thin-plate splines are composed of automatically generated and increasingly
wiggly basis functions that are defined over the entire range of the data
(Figure~\ref{fig:gam_dem}~(b)) and are then combined in the basis function
regression (Figure~\ref{fig:gam_dem}~(a)).

\textcolor{blue}{ When estimating the coefficients (i.e., weights) of the basis
  function regression, smoothing splines use an additional penalty term, }
similar to those used in a lasso or ridge regression, to control how closely
the smooth term fits the data during the estimation.
\textcolor{blue}{Optimizing the weight of this smoothing} penalty balances the
complexity and fit of the smooth term, ensuring the model captures the
underlying process accurately without overfitting
\parencite{gu_smoothing_2013, wahba_spline_1980}.

GAMs do not have a specific parameter, like the bandwidth in an LPR or the
lengthscale in a GP, that directly reflects the wiggliness of a process.
Instead, GAMs indicate the wiggliness of each smooth term through their
respective effective degrees of freedom (EDF), which measure the complexity of
each smooth term. Similar to the degrees of freedom in a regression model, the
EDF are tied to the number of coefficients and, therefore, the number of basis
functions in a smooth term. However, due to the penalty applied in GAMs, many
of these coefficients are constrained and cannot vary independently. This
reduces the effective degrees of freedom for each smooth term. Depending on the
weight of the penalty, the EDF can range from one (indicating a linear smooth)
to the total number of basis functions, with larger EDF values generally
reflecting a wigglier smooth term. However, unlike LPR and GPs, GAMs have the
advantage that they do not assume constant wiggliness in the underlying
process. This flexibility arises because the penalty term in a GAM only
captures the overall wiggliness of the estimated process\footnote{Typically in
  the form of the integrated squared second derivative of the estimated
  process.}
and, thus, does not constrain the wiggliness to be constant over time. For
example, an estimated process may have low wiggliness at the beginning and high
wiggliness at the end, yet still have the same overall penalty as another
estimate with consistent moderate wiggliness throughout the entire range.

\textcolor{blue}{
  The standard error estimates provided by GAMs commonly assume that the weight
  of the penalty term is fixed, whereas in reality it is most frequently
  optimized in a data driven manner. Beacuse of this, the standard errors
  tend to underestimate the uncertainty associated with the process estimate.}
However, simulations have shown that,
except when estimating process that are close to linear, the standard errors
remain reasonably accurate, resulting in confidence intervals with close to
nominal coverage proportions \parencite{marra_coverage_2012}. Additionally, the
accuracy of the standard errors could be improved through, for example,
Bayesian estimation, which accounts for the uncertainty in the penalty weight
\parencite{wood_generalized_2006}.

\subsubsection{\textcolor{blue}{Practical implementation}}

\textcolor{blue}{
  When applying GAMs, researchers need to select a spline basis, determine the
  number of basis functions, and optimize the smoothing penalty weight. For
  most applications, thin-plate or cubic regression splines are optimal default
  choices. The number of basis functions should be as large as possible
  (corresponding to the number of data points minus identifiability
  constraints) to ensure that the smoothing spline is sufficiently flexible to
  capture the underlying process accurately. Figure~\ref{fig:gam_dem}~(c) shows
  a smoothing spline with too few basis functions, leading to underfitting.
  However, using too many basis functions does not similarly result in
  overfitting (Figure~\ref{fig:gam_dem}~(d)), as the smoothing penalty can
  control the excessive model complexity. Nevertheless, a large number of basis
  functions can make models computationally expensive or infeasible for larger
  data sets. To address this, dimension reduction techniques similar to
  principal component analysis can approximate a smoothing spline solution
  using fewer basis functions \parencite{wood_thin_2003}. In this approach, the
  number of basis functions should be sufficient to capture the underlying
  process while remaining computationally practical. This procedure is
  automated in standard software \parencite{R-mgcv_a}; however, it is not used
  in this paper.}

\textcolor{blue}{
  Lastly, the smoothing penalty weight must be optimized to avoid over- or
  underfitting. A penalty that is too small results in a smoothing spline that
  is overly flexible and prone to overfitting (Figure~\ref{fig:gam_dem}~(d)).
  Conversely, a penalty that is too large yields a spline that is not flexible
  enough, leading to underfitting (Figure~\ref{fig:gam_dem}~(3)). The smoothing
  penalty weight is typically optimized through data-driven methods, such as
  directly maximizing the likelihood or using cross-validation.
}

\textcolor{blue}{ GAMs are implemented in the mgcv package
  \parencite{R-mgcv_a}. The model shown in Figure~\ref{fig:gam_dem}~(b) was
  implemented using the code {\fontsize{10}{12}\selectfont\texttt{ gam(y $\sim$
        s(time, bs = 'tp', k = 10)) }}, which fits the observed data with a
  single
  smooth term for time. The syntax specifies that the smooth term should be
  composed of ten thin-plate spline basis functions, although the number of
  basis functions can also be set to a larger value or determined
  automatically. The smoothing penalty weight is automatically optimized in
  mgcv to minimize the generalized cross-validation error. Similar to GPs, the
  mgcv model syntax allows for the creation of more sophisticated models by
  combining multiple smooth terms or incorporating parametric components. }

\textcolor{blue}{
  Fitting a GAM provides an estimate of the underlying process along with
  corresponding credible intervals. Additionally, researchers obtain the EDF
  for
  each smooth term separately. While the EDF are useful for assessing the
  wiggliness of a smooth term, it is important to note that they are not model
  parameters. Consequently, unlike GPs, GAMs do not provide a measure of
  uncertainty for the wiggliness of the process. The mgcv implementation of
  GAMs
  also includes significance tests to determine whether a given smooth term is
  effectively zero. Lastly, it is possible to extract the individual basis
  function coefficients from a fitted GAM.}

\subsection{Key differences between the methods}

A key difference between the three non-linear methods introduced in this paper
is the information they provide about the wiggliness of the underlying process.
Each method measures different aspects of the wiggliness and the interpretation
of the wiggliness estimates of each method depends on the chosen respective
configurations. For instance, the interpretation of the bandwidth of an LPR
changes depending on the selected polynomial degree and kernel. Similarly, the
interpretation of the lengthscale of a GP regression changes depending on the
chosen mean function and covariance kernel. This makes it almost impossible to
compare the wiggliness estimates between the presented methods and even between
different configurations of the same method. Other important differences
between the methods are summarized in Table~\ref{tab:meth_sum}.

\begin{table}[htbp]
  \begin{center}
    \begin{threeparttable}
      \caption{A comparison of LPR, GP regression and GAMs}
      \label{tab:meth_sum}
      \begin{singlespace}
        \begin{tabularx}{\linewidth}
          {>{\raggedright}s
            >{\raggedright}X
            >{\raggedright}X
            >{\raggedright\arraybackslash}X}
          \toprule
                                                            &
          \multicolumn{1}{c}{LPR}                           &
          \multicolumn{1}{c}{GP}                            &
          \multicolumn{1}{c}{GAM}
          \\
          \midrule
          Advantages                                        &
          \begin{itemize}
            \item Intuitive theory
            \item Completely data driven
          \end{itemize}                      &
          \begin{itemize}
            \item Interpretable parameters
            \item Natural uncertainty quantification
            \item Can incorporate (incomplete) prior theory
            \item Follows standard Bayesian analysis steps
          \end{itemize}   &
          \begin{itemize}
            \item Intuitive theory
            \item Some interpretable parameters
            \item Can incorporate (incomplete) prior theory
          \end{itemize}
          \\ \midrule
          Disadvantages                                     &
          \begin{itemize}
            \item Least interpretable parameters
            \item Biased for most processes
            \item No uncertainty estimate for the wiggliness
          \end{itemize}  &
          \begin{itemize}
            \item Difficult to specify in practice
          \end{itemize}            &
          \begin{itemize}
            \item No uncertainty estimate for the wiggliness
            \item Standard errors may be underestimated
          \end{itemize}
          \\ \midrule
          Required Choices                                  &
          \begin{itemize}
            \item Polynomial degree
            \item Kernel
            \item Optimization criterion
          \end{itemize}                      &
          \begin{itemize}
            \item Covariance kernel
            \item Mean function
            \item Hyperpriors
          \end{itemize}                           &
          \begin{itemize}
            \item Spline basis
            \item Number of basis functions
            \item Optimization criterion
          \end{itemize}
          \\ \midrule
          Key assumptions                                   &
          \begin{itemize}
            \item P-times differentiable process
            \item Constant wiggliness
          \end{itemize}              &
          \begin{itemize}
            \item Assumptions depend on chosen specifications
          \end{itemize} &
          \begin{itemize}
            \item Smooth process
            \item Homoscedasticity
          \end{itemize}
          \\ \midrule
          Estimation                                        &
          \begin{itemize}
            \item OLS
          \end{itemize}                                   &
          \begin{itemize}
            \item Bayesian, Marginal likelihood maximization
          \end{itemize}  &
          \begin{itemize}
            \item OLS, MLE, Bayesian
          \end{itemize}
          \\ \midrule
          Key sources of information                        &
          \begin{itemize}
            \item \textcite{fan_local_2018}
          \end{itemize}                   &
          \begin{itemize}
            \item \textcite{rasmussen_gaussian_2006}
          \end{itemize}          &
          \begin{itemize}
            \item \textcite{wood_generalized_2006}
          \end{itemize}
          \\
          \bottomrule
        \end{tabularx}
      \end{singlespace}
    \end{threeparttable}
  \end{center}
\end{table}

\section{Simulation} \label{simulation}

\subsection{Problem}

A simulation study was conducted to assess the effectiveness of the introduced
methods in recovering different non-linear processes that may be encountered in
EMA research (Figure~\ref{fig:examplar_npn}). In this simulation, the three
methods were not only compared against each other but also to a polynomial
regression model (the current most used method to model non-linear trends in
psychology) and to parametric models that accurately specify the respective
non-linear processes. These (data-generating) parametric models were added to
serve as a benchmark for the non-linear process recovery. To apply the
introduced methods in line with how they were introduced and within the
constraints of available software implementations, the simulation focused on a
univariate single-subject design. Hence, the simulated data represented
repeated measurements of a single variable for one individual.

\subsection{Design}

To conduct the simulation with processes that might be encountered in real EMA
studies, we selected the exemplar processes illustrated in
Figure~\ref{fig:examplar_npn} as a basis. These include two growth curves
(modeled as an exponential and a logistic growth curve), a mean-level switching
process (modeled as a cusp catastrophe), and a self-regulatory process
(represented by a damped oscillator).

Since real live processes are likely affected by external influences (i.e.,
context effects), we account for these influences in the simulation by adding
dynamic errors to each process. These dynamic errors reflect external
perturbations (or other forms of unaccounted influence) to the latent construct
that are carried forward in time. For instance, if a participant experiences an
unusually pleasant conversation that elevates their true positive affect, this
change represents an error effect if it is not accounted for by the model.
However, since the true positive affect level has increased, future
measurements will be influenced due to, for example, emotional inertia. As
such, these dynamical errors make the processes rougher/less smooth (i.e.,
non-differentiable; Figure~\ref{fig:exemplar_pn}). The degree of roughness was
controlled by the variance of the dynamic errors. To demonstrate the varying
roughness of the processes, Figure~\ref{fig:exemplar_pn} presents a possible
realization of each process with a dynamic error variance of 0.5 (left) and
with a variance of 2 (right). We considered variances of 0.5, 1, and 2
reasonable relative to the process range. \textcolor{blue}{ Additionally,
  Figure~\ref{fig:stoch_mean_var} displays the means and variances of 100 data
  sets sampled from each of the stochastic processes for varying dynamic error
  variances. } Importantly, we intentionally omitted a condition without
dynamic
errors from this simulation, as dynamic errors are reasonably expected to be
present in all psychological ILD\@.

\begin{figure}[!t]
  \caption{Simulation conditions}
  \fitfigure{exemplar_process_noise.png}
  \figurenote{This figure illustrates the different conditions that were
    manipulated in the simulation. It shows a possible realization of each
    exemplar process with a dynamic error variance of 0.5 (left) and 2 (right).
    Further, it shows how the sampling period was manipulated by sampling
    either only over the first half (black dots) or over the entire period of
    each process (black and blue dots). Lastly, the sampling frequency was
    manipulated. The top four panels display samples with one observation per
    time step, whereas the bottom four panels display samples with three
    observations per time step.}
  \label{fig:exemplar_pn}
\end{figure}

\begin{sidewaysfigure*}[htbp]
  \caption{Pointwise means and standard deviations of the simulated stochastic
    processes}
  \fitfigure{stoch_mean_variance.png}
  \label{fig:stoch_mean_var}
  \figurenote{This figure depicts the pointwise means and standard deviations
    of 100 data sets simulated from each of the stochastic processes, with
    dynamic error variances of 0.5, 1, and 2. The solid black line depicts the
    mean of all simulated data sets at each time point. The shaded red areas
    depcit 67, 95, and 99.7 percent of the simualted data around the mean at
    each time point.
  }
\end{sidewaysfigure*}

Additionally, we varied both the sampling period (i.e., the duration over which
the process is measured) and the sampling frequency (i.e., the number of
measurements taken during that period) for each process. Since the data in the
simulation is generated, it does not have an inherent time scale. This means
that one time step in the simulated data could represent an hour, a day, or a
year, and that the time scaling of the simulated processes is only meaningful
relative to the scale at which each process exhibits its characteristic
behavior. For instance, if the chosen time scale is too long, both growth
curves (Figure~\ref{fig:examplar_npn}~a and b; with the chosen parameters)
would display a brief period of growth followed by a long, nearly flat phase
approaching the asymptote. To ensure consistency, all processes were simulated
to display their characteristic behavior over the same period.

To simulate measuring each process for a shorter or longer time, thus capturing
different behaviors of each process, data was either generated over only the
first half (Figure~\ref{fig:exemplar_pn}, black dots) or over the entire period
(Figure~\ref{fig:exemplar_pn}, black and blue dots). For example, sampling the
two growth curves only over the first half of the period would mean that the
process is not measured close to the asymptote.

\textcolor{blue}{
  Lastly, we manipulated the sampling frequency by varying the number of
  equidistant measurements taken per time step. As a result of this simulation
  design, the overall sample size in each condition is determined by the
  combination of the sampling period and the sampling frequency. For example,
  increasing the sampling frequency within a fixed period by a factor of two
  halves the interval between consecutive measurement points and, in turn,
  approximately doubles the overall sample size. Similarly, doubling the
  sampling period also doubles the overall sample size without changing the
  distance between measurements. Sampling frequencies of one, two, and three
  observations per time step produced sample sizes typical of EMA studies,
  ranging between 43 and 253 total observations
  \parencite{wrzus_ecological_2023}.
} This is illustrated in
Figure~\ref{fig:exemplar_pn} where the top four panels show examples with one
observation per time step, whereas the bottom four panels illustrate three
observations per time step, simulating a scenario where measurements are taken
three times more frequently. Overall, this resulted in sample sizes ranging
between 42 observations (one observation per time step for half the period) to
252 observations (three observations per time step for the entire period).

\subsection{Hypotheses}

Since the parametric models matched the true data-generating models, we
expected them to infer the underlying processes most accurately, serving as a
benchmark for comparison with the other methods. We also anticipated that the
(global) polynomial regression would infer all processes less accurately than
the three introduced approaches because of the limitations outlined in the
introduction. However, we did not have specific hypotheses regarding the
relative performance of the three non- and semi-parametric methods.

Additionally, we anticipated that all methods, except the parametric models,
would struggle to accurately infer the cusp-catastrophe process, as each method
produces continuous estimates that may be inadequate for capturing the apparent
jumps in the process. Similarly, because all methods (except the parametric
models) produce smooth estimates using the default configurations in which they
are most often applied and implemented in standard software, we expected the
performance of all methods to decline with larger dynamic error variances.

Our expectations regarding the sampling period were more nuanced. The LPR and
GP assume that the wiggliness (as defined by each non-parametric method
respectively) of the process remains constant over time. However, all four
processes exhibit changes in wiggliness. For example, the wiggliness of the
exponential and logistic growth functions and the damped oscillator decreases
monotonically, while the cusp-catastrophe's wiggliness fluctuates cyclically
(i.e., low wiggliness during plateau phases and high wiggliness during jumps).
Therefore, we hypothesized that longer sampling periods for the exponential and
logistic growth curves and the damped oscillator would reduce the inference
accuracy of the LPR and GP as the single bandwidth or lengthscale parameter
becomes increasingly inadequate to capture the changing wiggliness over time.
Furthermore, the LPR was not expected to benefit from the additional
data provided by measuring the process for a longer time, since it relies
mostly on data in its local neighborhood during the estimation. Therefore,
extending the sampling period beyond these local neighborhoods was not expected
to increase the inference accuracy. In contrast, the GAMs and parametric
models, which do not assume constant wiggliness and utilize the entire data set
during the estimation, were expected to infer all processes more accurately
with larger sampling periods.

Lastly, we predicted that increasing the sampling frequency would generally
improve the inference accuracy across all methods by providing more information
about the underlying processes.

\subsection{Outcome measures}

To evaluate and compare the performance of the different analysis methods, we
focused on three outcome measures: the mean squared error (MSE), the
generalized cross-validation criterion (GCV), and the confidence interval
coverage. To assess how effectively each method captured the non-linear process
at the observed time points, we calculated the MSE between the estimated and
generated process values. Additionally, to evaluate how well each method would
predict omitted process values within the process range, we computed the GCV
\parencite{golub_generalized_1979} criterion for each method and data set. The
GCV is a more computationally efficient and rotation-invariant version of the
ordinary leave-one-out cross-validation criterion with the same interpretation.
Both, the GCV and the leave-one-out cross-validation criterion, describe how
accurately the model predicts omitted data points within the design range,
which provides information about how well the model interpolates the process.
\textcolor{blue}{However, in contrast to the ordinary leave-one-out
  cross-validation, the GCV does not require each model to be refit to many
  data
  subsets. Instead it utilizes that for all presented methods, a linear
  function
  exists that maps the observed data points to the predicted process values
  (i.e., the influence, projection, or hat matrix). This linear function can be
  used to analytically calculate how the predicted values would change if any
  of
  the data points were removed from the model, which makes it possible to
  calculate a cross-validation criterion, without the computationally intensive
  refitting of the model.}

To evaluate the uncertainty estimates provided by each method, we recorded
whether the true generated process was located within the confidence or
credible intervals at each time point. Subsequently, the average confidence
interval coverage proportion for each method and data set was obtained by
averaging over all time points. Given that all confidence or credible intervals
were set at a 95\% confidence level, the expected coverage proportion should
ideally be close to 95\%. Due to Monte Carlo error in the simulation
($\max(se_{CIC}) \approx 0.03$), individual average confidence interval
coverages are expected to deviate from the ideal 95\%. Because of this, average
coverage proportions between 89\% and 100\% were also deemed acceptable.
Average coverage proportions below 89\% may indicate (but may not only be due
to) underestimated uncertainty.

\subsection{Data generation}

Each process in Figure~\ref{fig:exemplar_pn} was represented as a generative
stochastic differential equation model. These dynamic models describe the
relationship between the process's current value and its instantaneous rate of
change. Combined with information about the initial state of the process this
makes it possible to describe the entire process indirectly. For instance, the
stochastic differential equation model used to represent the introduced
logistic growth process can be expressed as follows:

\begin{equation} \label{eq:2}
  dy = r y (1-\frac{y}{k})dt + \sigma dW_t
\end{equation}

\noindent The first half of this model defines the deterministic dynamics of
the process. It relates the rate of change of $y$ to its current value and to
how far away the current value is from the asymptote $k$ through a growth rate
constant $r$. The second part of the model accounts for the dynamic errors in
the form of a Wiener process. The Wiener process is a continuous
non-differentiable stochastic process, which describes normally distributed
dynamic errors over any given discrete time interval. These errors have a mean
of zero and a variance depending on the length of the time interval and
$\sigma$, making them optimal for this simulation. Importantly, these dynamic
errors continuously influence the rate of change of the process and are
propagated forward in time through the deterministic dynamics of the model.
Simulating the other processes was achieved by simply using other equations for
the deterministic dynamics in Equation~\ref{eq:2}. The precise equations used
for the other methods are detailed in the online supplementary material.

The resulting processes were then simulated using the Euler-Maruyama method,
which approximates stochastic differential equation systems with an arbitrarily
high accuracy by linearizing them over small discrete time intervals. The
resulting high-resolution data were then subsampled to achieve the desired
sampling frequency. Finally, measurement errors were added to the latent
process data at each time point independently from a standard normal
distribution, generating the final sets of observations.

Based on an initial pilot sample of 30 data sets per condition, we determined
the number of replications needed to achieve a Monte Carlo standard error of
less than 0.03 for the confidence interval coverage \textcolor{blue}{for all
  methods, models, and simulation conditions}
\parencite{siepe_simulation_2023}. These Monte Carlo standard errors reflect
the expected variation in the outcome statistics due to random processes within
the simulation. This analysis showed that 100 replications per condition
would result in maximal Monte Carlo standard errors of approximately $se_{MSE}
  \approx 0.05$, $se_{GCV} \approx 0.38$, and $se_{CIC} \approx 0.03$.

\subsection{Model estimation}

After simulating the data, all introduced methods were applied to each data set
using the statistical software R \parencite{R-base}. The specific
configurations used for each method were chosen to reflect how each method is
most commonly applied in practice and not to optimally infer the simulated, and
thus known, processes. First, the LPR was estimated using the nprobust package
\parencite{R-nprobust}, which can correct for the bias inherent in LPRs.
Second, GPs were estimated in STAN\footnote{Alternatively, the GP regression
  models used in this article can also be fit using the BRMS package
  \parencite{R-brms}.}
\parencite{R-cmdstanr} with a zero mean and a squared exponential covariance
function, following common practice. Third, GAMs with a single smooth term for
time were fitted using the mgcv package \parencite{R-mgcv_a}. The polynomial
regressions were estimated using base R, with correlated (i.e., standard)
polynomial terms and data set specific polynomial degrees. Finally, the
parametric stochastic differential equation models corresponding to the true
data-generating models were estimated using the Dynr package
\parencite{R-dynr}. After fitting each model to the data, they were used to
obtain point and interval estimates (i.e., 95\% confidence and credible
intervals) for the latent process at each time point. A detailed description of
each model fitting procedure is provided in the online supplementary material.
Further, if any models failed to converge during the simulation, the
corresponding outcome measures were excluded from the following analyses.

\textcolor{blue}{
  The R code used to simulate the data, fit the models, obtain
  the results, and generate the figures is available in the
  \href{https://osf.io/3ytdx/}{online supplementary material}. Additionally, a
  \href{https://jan-ian-failenschmid.shinyapps.io/modeling_non_linearity_app/}{
    supplementary web application } is provided, allowing readers to further
  inspect and explore the results presented in the paper. The web application
  also enables users to simulate data based on any of the processes discussed
  in
  the paper and fit some of the computationally less intensive models.
}

\subsection{Results}

In the simulation, a small proportion of GP regressions
\textcolor{blue}{(1.43\%)} and parametric models \textcolor{blue}{(28.60\%)}
did not converge\footnote{\textcolor{blue}{Further information regarding the
    proportion of non-converging models in each condition can be found in the
    supplementary web app.}}. This is most likely due to the small
sample
sizes considered
and the automated model fitting in the simulation. Most notably, the parametric
model was not able to infer the cusp catastrophe from the small simulated data
sets due to the complexity of the model. The performance measures of the
methods that did not converge were removed from the following analysis.
Additionally, the parametric models appear to have overfit for a small number
of data sets from all processes. The complete simulation results and data are
available in the online supplementary material.

\subsubsection{Capturing the non-linear process}

It is noteworthy that the point estimates produced by all considered methods
were able to visually follow the simulated processes adequately.
Figure~\ref{fig:smooth} illustrates an example of each process being inferred
by all methods. While the predicted means produced by each method all follow
the overall trajectory of the processes, several observations can be made based
on this figure. First, the local and the global polynomial regression appear to
underfit (i.e., oversmooth) for some of the processes. Second, the GP
regression produced confidence intervals that are notably narrower than the
uncertainty estimates produced by the other methods. Third, while the global
polynomial regression generally underfit the processes, it appears to overfit
near the boundary (i.e., the beginning and end of the timeseries) resulting in
excessive uncertainty. This is a known problematic behavior of the global
polynomial regression. Lastly, all methods appear to struggle with accurately
capturing the cusp-catastrophe, especially its nearly stable periods, often
producing estimates that resemble sine functions. However, there is
considerable variation and overlap in the accuracy and behavior of the
different methods across the various data sets, which highlights the need for a
more formal analysis of the performance of each method.

\begin{sidewaysfigure*}[htbp]
  \caption{Example processes inferred by each of the introduced methods}
  \fitfigure{smooth.png}
  \label{fig:smooth}
  \figurenote{This figure shows how each of the introduced methods inferred an
    example of each of the processes from the simulation.\textcolor{blue}{ In
      this figure, the black line shows the true simulated process, while the
      solid red line shows the estimated process. The dashed red lines around
      the
      estimated process display 95\% confidence or credible bands. No estimates
      are displayed in the last panel since the parametric models did not
      converge for any of the simulated data sets of the cusp catastrophe
      model.
    }}
\end{sidewaysfigure*}

To summarize the high dimensional results efficiently, two separate ANOVAs were
fitted to the MSE and GCV values, including all possible main and interaction
effects. Table~\ref{tab:peta} presents all effects for which the
partial-$\eta^2$, indicates at least a small effect size for either the MSE or
the GCV\@. The most influential effects for both the MSE and GCV belonged to
the analysis method, the data-generating process, and the dynamic error
variance. The main effects of the sampling period and sampling frequency were
considerably smaller and comparable in magnitude to some of the first-degree
interaction effects. The following sections will focus on describing the most
important of these effects. A comprehensive overview of all effects in the
models can be found in the online supplementary material.

\begin{table}[tbp] % import(tables/eta_squared.txt)

  \begin{center}
    \begin{threeparttable}
      \caption{Effect sizes from the MSE and GCV ANOVAs}
      \label{tab:peta}
      \begin{tabular}{lll}
        \toprule
        Effect                & \multicolumn{1}{c}{partial-$\eta^2$ MSE} &
        \multicolumn{1}{c}{partial-$\eta^2$ GCV}
        \\
        \midrule
        Method                & 0.39                                     & 0.25
        \\
        Process               & 0.54                                     & 0.40
        \\
        SP                    & 0.17                                     & 0.08
        \\
        SF                    & 0.15                                     & 0.23
        \\
        DEV                   & 0.56                                     & 0.48
        \\
        Method:Process        & 0.22                                     & 0.11
        \\
        Method:SP             & 0.16                                     & 0.08
        \\
        Process:SP            & 0.06                                     & 0.04
        \\
        Method:SF             & 0.05                                     & 0.01
        \\
        Process:SF            & 0.01                                     & 0.04
        \\
        Method:DEV            & 0.18                                     & 0.10
        \\
        Process:DEV           & 0.27                                     & 0.23
        \\
        SP:DEV                & 0.02                                     & 0.02
        \\
        SF:DEV                & 0.01                                     & 0.05
        \\
        Method:Process:SP     & 0.07                                     & 0.05
        \\
        Method:Process:SF     & 0.02                                     & 0.01
        \\
        Method:SP:SF          & 0.01                                     & 0.00
        \\
        Method:Process:DEV    & 0.08                                     & 0.05
        \\
        Method:SP:DEV         & 0.02                                     & 0.03
        \\
        Process:SP:DEV        & 0.01                                     & 0.01
        \\
        Process:SF:DEV        & 0.00                                     & 0.02
        \\
        Method:Process:SP:DEV & 0.01                                     & 0.02
        \\
        \bottomrule
      \end{tabular}
      \tablenote{This table shows all effects from the MSE and GCV ANOVA that
        had at least a small effect partial-$\eta^2 >= 0.01$ on either
        outcome. SP\@: Sampling period; SF\@: Sampling frequency; DEV\@:
        Dynamic error variance.}
    \end{threeparttable}
  \end{center}
\end{table}

The mean MSE results are depicted in Figure~\ref{fig:mean_results_mse}. Panel
(a) shows the average MSE with which each method inferred each of the
processes, when averaging over all other conditions. There appear to be clear
differences in the average MSE with which each method infers all processes on
average, illustrating the main effect of the analysis method. Specifically, the
parametric modeling showed the lowest average MSE, followed closely by the
GAMs, whereas the GP regression, LPR, and the polynomial regression had larger
average MSE values. Similarly, the main effect of the process can be seen in
that some processes were inferred with a lower average MSE by all methods. Most
notably, the cusp catastrophe was inferred with lower MSE values than the other
processes across all methods. Lastly, panel (a) also depicts the interaction
effect between the methods and the processes, since the mean MSE differences
between the different analysis methods are not equal across the different
processes. For example, the polynomial regression displayed a noticeably larger
mean MSE for all processes except the cusp catastrophe in comparison to the
more advanced statistical methods.

\begin{sidewaysfigure*}[htbp]
  \caption{Mean MSE effects across all processes, analysis
    methods, and simulation conditions}
  \fitfigure{mean_results_mse.png}
  \figurenote{Panel (a) shows the effect of the analysis method for each
    process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_mse}
\end{sidewaysfigure*}

Figure~\ref{fig:mean_results_mse}~(b) illustrates the effect of increasing
dynamic error variances for all methods and processes (averaged over all
sampling periods and frequencies), which is the largest main effect among the
simulation conditions. A larger dynamic error generally led to larger average
MSE values. However, this effect was considerably less pronounced for the
parametric models, GPs, GAMs, and the cusp-catastrophe process, highlighting
the interaction between the dynamic error variance and the method or process,
respectively. Panel (c) shows the effect of the sampling period for each method
and process (averaged over all dynamic error variances and sampling
frequencies). Here, it can be seen that sampling over the entire period, rather
than just the first half, resulted in larger mean MSE values for the local and
global polynomial regression for all processes except the
cusp-catastrophe. This effect was (almost) absent for the GP regression,
absent for the GAMs, and reversed for the parametric models. Lastly,
Figure~\ref{fig:mean_results_mse}~(d) shows that the mean MSE generally
decreased with larger sampling frequencies for each method and process
(averaged over all dynamic error variances and sampling periods).

Figure~\ref{fig:mean_results_gcv} displays the corresponding effects for the
mean GCV values. Similar to the MSE results, the GAMs show a mean GCV value
closest to the benchmark parametric models. However, in terms of the mean GCV,
the GP regressions perform equally as well as the GAMs. The local and global
polynomial regressions show considerably larger mean GCV values for all
processes except the cusp catastrophe. The effects of the dynamic error
variance, measurement period, and frequency on the mean GCV appear to follow
largely the same patterns that were observed for the mean MSE\@.

\begin{sidewaysfigure*}[htbp]
  \caption{Mean GCV effects across all processes, analysis
    methods, and simulation conditions}
  \fitfigure{mean_results_gcv.png}
  \figurenote{Panel (a) shows the effect of the analysis methods for each
    latent process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_gcv}
\end{sidewaysfigure*}

\subsubsection{Uncertainty quantification}

Figure~\ref{fig:mean_results_ci_coverage} shows the average confidence interval
coverage proportion for the conditions described above. The grey area
represents an average confidence interval coverage between 89\% and 100\%,
which indicates no considerable deviation from the ideal 95\% given the Monte
Carlo error of the simulation. Only the parametric models produced some mean
confidence interval coverages within this area. Among the other methods, the
GAMs, produced the largest average confidence interval coverage, followed by
the global and then the local polynomial regression. The GP regression appears
to result in the smallest average confidence interval coverage.

\begin{sidewaysfigure*}[htbp]
  \caption{Average confidence interval coverage across all processes, analysis
    methods, and simulation conditions}
  \fitfigure{mean_results_ci_coverage.png}
  \figurenote{Panel (a) shows the effect of the analysis methods for each
    latent process. The other three panels show the effects of measurement
    period (b), measurement frequency (c), and dynamic error variance (d) for
    each analysis method and latent process.}
  \label{fig:mean_results_ci_coverage}
\end{sidewaysfigure*}

\subsection{Conclusion}

The simulation showed that the GAMs inferred all processes with the most
accuracy among the considered methods, in their default configuration, as
indicated by the mean MSE, GCV and confidence interval coverage. The GAMs
performed closest to the true data generating models, followed by the GP (with
regards to the MSE and GCV, as the GP had the lowest confidence interval
coverage), and then the local and global polynomial regression. We conclude
that GAMs are an attractive starting point for modeling unknown non-linear
processes in ILD, especially when there is little prior theory about the
functional form of the process.

Additionally, the simulation revealed that larger dynamic error variances
reduced the accuracy of all methods. Therefore, addressing sources of dynamic
error in practice is recommended. This can be achieved, for example, by
measuring and accounting for contextual variables and other sources of external
disturbances. The results also indicate that increasing the sampling frequency
improved the performance of all methods, making it generally advantageous to
measure processes more frequently. However, in practice, this must be weighed
against considerations like participant burden and fatigue, which can
negatively impact data quality. Lastly, the simulation showed that extending
the sampling period enhanced the performance of the parametric models but may
decrease the accuracy of LPR and polynomial regression. This reduction in
accuracy for the LPR could potentially be mitigated by using extensions that
allow for variable bandwidths or polynomial degrees.

\section{An Empirical Example} \label{empirical_example}

In the following, we applied GAMs to depression data from the Leuven clinical
study, which were obtained from the EMOTE database
\parencite{kalokerinos_emote_nodate}. We selected the data for their
heterogeneous sample, which contains momentary depression scores of
participants who met the DSM criteria for mood disorders or borderline
personality disorder during an intake, as well as depression scores for healthy
controls. For a more thorough sample and data description, including screening
protocols, see \textcite{heininga_dynamical_2019}. The diversity in the study
population makes it likely to find non-linear processes for at least some
participants.

The data set used for this application contained 77 participants in the
clinical
sample and 40 participants in the control sample, who were matched to the
clinical sample by gender and age, resulting in a total sample size of
117\footnote{The original published data set contained one additional
  participant who was removed for this analysis since they had a depression
  score
  of zero across all assessments.}. The participants completed seven days of
semi-random EMA assessments, with 10 equidistant assessments per day. During
each assessment, participants responded to the item `How depressed do you feel
at the moment?' on a scale ranging from 0 to 100 to assess their momentary
depression\footnote{Note that this was only one of 27 items that were assessed
  at every measurement occasion. The other items (questions about emotions,
  social expectancies, emotion regulation, context, and psychiatric symptoms)
  are, however, not relevant to this application and, therefore, not further
  described.}.

The initial study procedure was approved by the KU Leuven Social and Societal
Ethics Committee and the KU Leuven Medical Ethics Committee. This secondary
data analysis was approved by the Ethics Review Board of the Tilburg School of
Social and Behavioral Sciences (TSB RP FT16). The complete code for this
analysis can be found in the online supplementary material.

\subsection{Analysis and Results}

To maintain consistency with how the GAMs were introduced in this article and
used in the simulation, we applied GAMs with a single smooth term to explore
the idiographic processes underlying the data of each individual. Inspecting
the estimated processes revealed a clear picture of the heterogeneity in the
processes underlying the data. On the one hand, Figure~\ref{fig:dem_smooth}~(a)
shows the ten estimated processes with the lowest EDF, corresponding to the
least wiggly estimated processes, which appear to be essentially linear. In
fact, for 72 out of 117 participants, the process inferred by the idiographic
GAMs is effectively a linear trend ($EDF < 1.001$). For some of these
participants, the linear estimates appear to be due to strong floor effects
where participants repeatedly indicate depression scores close to zero.
However, this explanation does not hold for all participants, as some
participants also received linear estimates without displaying floor effects.

Figure~\ref{fig:dem_smooth}~(b), on the other hand, shows the ten participants
with the wiggliest estimated processes, indicated by the largest EDF (between
5.44 and 19.32). For these participants, the estimated processes clearly
deviate from a linear trend. Notably, visual inspection of the estimated
non-linear processes did not reveal any common behaviors across the processes
of different participants. With respect to the processes introduced in this
article, two participants show particularly interesting trajectories. Their
trajectories are shown in Figures~\ref{fig:dem_smooth}~(c) and (d),
respectively. The process in Figure~\ref{fig:dem_smooth}~(c) closely resembles
the decreasing oscillations exhibited by a damped oscillator. The process in
Figure~\ref{fig:dem_smooth}~(d) shows a participant who may be switching
between a low and a high depression state, with considerable oscillations
within each state.

\begin{figure}[!t]
  \caption{Estimated depression processes}
  \fitfigure{demonstration_smooths.png}
  \label{fig:dem_smooth}
\end{figure}

\subsection{Conclusion}

This application demonstrates how GAMs can be used to explore potentially
non-linear idiographic processes and identify possible empirical phenomena. In
this depression data, the most notable finding is that the GAMs inferred a
linear trend for most participants but detected non-linear trajectories for
some individuals. For individuals with a linear trend, the results suggest the
absence of dynamic errors, which would otherwise result in deviations from the
linear trend. This could, for instance, indicate that these participants are
not reactive to external influences. Similarly, for individuals with non-linear
trends, the observed deviations from a linear trend could also be explained by
stronger dynamic errors in these individuals. In either case, the results of
this application suggest considerable heterogeneity within the sample, whether
in the linearity of idiographic trends or individual reactivity to external
influences. This highlights the importance of using non-parametric methods to
explore idiographic processes, as a misspecified parametric model would likely
obscure such qualitative differences, and insights from non-parametric
exploration can offer valuable guidance for future theories.

\section{Discussion}

In this paper, we introduced three advanced semi- and non-parametric regression
techniques for estimating non-linear processes in psychological ILD
(Section~\ref{method_introduction}). These methods address many of the
limitations inherent in polynomial and piecewise spline regression, which are
currently the most common approaches for modeling non-linearity in psychology.
A simulation study (Section~\ref{simulation}) further showed that the
introduced methods inferred the types of non-linear processes that were
previously found in psychological ILD more accurately than a polynomial
regression. Particularly, GAMs closely approximated the underlying processes,
performing almost as well as the data-generating parametric models. Finally, we
showed how GAMs can be used to explore idiographic processes and identify
potential phenomena from data (Section~\ref{empirical_example}). This
comprehensive analysis empowers psychological researchers to accurately model
non-linear processes and to select analysis methods that align with their
research goals and data characteristics.

The results obtained in the simulation aligned with almost all of our prior
hypotheses. The first hypothesis that was falsified is that the
cusp-catastrophe process was inferred most accurately by all methods. We had
anticipated that the smooth, continuous estimates produced by all methods would
struggle to adapt to the jumps exhibited by this process. However, this effect
seems to have been overshadowed by the cusp-catastrophe's strong resilience to
external perturbations. This property is highlighted in
Figure~\ref{fig:exemplar_pn}, where dynamic errors with the same variance have
been applied to all four processes. Despite the perturbations being of equal
variance, the cusp-catastrophe model appears to be the least affected and even
closely resembles the unperturbed process (Figure~\ref{fig:examplar_npn}).
Further evidence of this can be seen in the simulation, where the effect of
increasing the dynamic error variance was weakest for the
cusp-catastrophe\footnote{Further, rerunning the simulation with considerably
  smaller dynamic error variances resulted in the cusp-catastrophe being
  inferred
  least accurately.}. The second hypothesis that was in partial misalignment
with
the findings is that we expected the GP to perform worse for longer sampling
periods (for the two growth curves and the damped oscillator). This effect was
only pronounced for the confidence interval coverage and (nearly) absent for
the mean MSE and GCV\@.

Regarding the presented simulation results, it is important to note that the
observed differences in performance may be mainly caused by the specific
configurations used for each method rather than the general methods themselves.
The specific configurations of each method were chosen to reflect how each
method is most commonly applied in practice and not to optimally infer the
simulated processes. Consequently, different configurations and extensions are
likely to improve the performance of the LPR and GP\footnote{Especially, since
  the GAMs used in this simulation can be expressed as a (unconventional) GP
  with
  a linear mean function and a non-stationary covariance function
  \parencite{wahba_improper_1978, rasmussen_gaussian_2006}, which implies that
  there exists a GP configuration which performs at least as well as the
  GAMs.}.
For example, using a GP with a Matern class kernel, which makes less strict
smoothness assumptions about the process, could be expected to improve the
accuracy for the rough processes considered in this simulation. Therefore, if
there is prior knowledge about the form of the process available in practice,
GPs are a very interesting modeling approach, due to their flexibility and
interpretable parameters. However, this likely still requires fine-tuning the
configurations of the GP to one's specific conditions. Another approach to
enhance the performance of GPs when estimating processes with changing
wiggliness, could be to employ a warp function that adjusts the distance
between time points based on their location. This approach makes it possible to
indirectly describe a time-varying lengthscale that can adapt to the changing
wiggliness of the process \parencite{tolpin_warped_2019}. Regarding the LPR,
several optimality results have been found indicating that LPR should be at
least as accurate as GAMs and GPs for processes that satisfy the smoothness
assumption of the LPR \parencite{fan_local_1997}, such as the linear processes
found in the presented empirical example. However, since it is typically
unknown whether a process is smooth a priori, it is unclear whether these
optimality results can be generalized to other psychological processes.

Lastly,
the tested polynomial regression inferred the underlying processes least
accurately, highlighting the limitations of this approach. The numerical model
instability of large polynomial regressions can be partially mitigated by using
orthogonal polynomials, which are more difficult to interpret. Based on
the results presented in this paper, there appears to be no reason to use a
global polynomial regression instead of one of the other presented methods in
any situation in which prior theory does not strongly suggest that the process
follows a low order polynomial trajectory.

The results presented in this article are accompanied by several important
limitations. First, only a limited number of processes were tested in the
simulation and it is possible that the presented results do not generalize to
other processes. While the theory underlying each method guarantees a good
performance for processes that satisfy their respective assumptions, it is
generally not known how each method performs for other processes that violate
their assumptions. Second, while it is necessary to use the GCV instead of the
classical leave-one-out cross-validation criterion to reduce the computational
load of the simulation, the GCV may inadvertently favor the GP regression and
the parametric models. This is because the analytic form of the GCV
implicitly requires certain parameters (such as the hyperparameters of the GP
covariance function) to be estimated on the entire sample and then be fixed (to
these values) during the cross-validation. For the GP and the parametric models
this likely improves their ability to predict the omitted data points.

Finally, the scope of this paper was restricted to univariate single-subject
data with independent normally distributed measurement errors. This somewhat
idealized scenario was chosen to introduce all methods accessibly within the
constraints of available software. However, this does not address many of the
goals and challenges researchers face when working with ILD, which
frequently contains complex measurements of several psychological variables for
multiple individuals. However, there are many extensions available for the
introduced methods, adapting them closer to the needs of ILD\@. For instance,
GAMs and GPs can be naturally extended to multilevel data
\parencite{karch_gaussian_2020, wood_generalized_2006}. Similarly, GP
regression can be used to model different relationships between the
observations and the latent process by changing their associated likelihood to
a psychometric model, such as a factor model \parencite{clark_dynamic_2023,
  yu_gaussian-process_2009}.

There are also many ways in which the present methods can be used to study
multivariate data and structural relations between different psychological
variables. The reason for this is that, even though all methods were introduced
in the context of inferring non-linear processes, they can theoretically be
used to estimate a wide range of non-linear functions. This makes it possible
to infer, for example, non-linear cross- and autoregressive relationships from
data in discrete time \parencite{bringmann_modeling_2015,
  wood_generalized_2006, rasmussen_gaussian_2006,
  eleftheriadis_identification_2017} and non-linear differential equation
models
in continuous time \parencite{yildiz_learning_2018}. These models also present
the exciting possibility to combine partial parametric models with non-linear
data-driven functions to test incomplete theories.

While these extensions and possibilities exist in theory and are individually
implemented in software, most are not yet available in a form that allows
researchers to flexibly and accessibly adapt these methods to the specific
characteristics of ILD\@. Therefore, it is essential that future research
unifies these methods and their extensions in software designed for the applied
modeling of ILD\@.

\printbibliography[]

\end{document}