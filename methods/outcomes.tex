% -----------------------------------------------------------------------------%
% Title:                                                                       %
% Author: Jan Ian Failenschmid                                                 %
% Created Date: 13-03-2024                                                     %
% -----                                                                        %
% Last Modified: 02-04-2024                                                    %
% Modified By: Jan Ian Failenschmid                                            %
% -----                                                                        %
% Copyright (c) 2024 by Jan Ian Failenschmid                                   %
% E-mail: J.I.Failenschmid@tilburguniveristy.edu                               %
% -----                                                                        %
% License: GNU General Public License v3.0 or later                            %
% License URL: https://www.gnu.org/licenses/gpl-3.0-standalone.html            %
% -----------------------------------------------------------------------------%

\subsection{Outcome measures}

To analyze and compare the performance of the different analysis methods, we
focused on three different outcome measures. The first two measured the
accuracy of each methods in predicting the value of
the process at or between the time points at which the observations
were taken. These measures of predictive accuracy provide insight into how
well the non-linear process is captured by each of the methods. The last
outcome measure, concerned the accuracy of the uncertainty estimates provided
by each of the methods. Here we investigated whether the confidence intervals
provided by each method indeed include the true state value the correct
proportion of times.

\subsubsection{Capturing the non-linear process}

In order to test how well each of the methods captured the non-linear process
in each condition, the root mean squared error (RMSE)
between the estimated and the generated values was calculated. Further, to
estimate how well each of the methods predicts the process between two observed
time-points, we calculated the generalized cross-validation
(GCV; \textcite{golub_generalized_1979}) criterion
for each method and data set. The GCV is a computationally more efficient and
rotation invariant version of the ordinary leave-one-out cross-validation
criterion with the same interpretation. The latter is calculated by
removing one data point and refitting the
model while keeping certain parameters fixed. Afterwards, the left out
observation is predicted and the squared prediction error calculated.
epeating this procedure for each data point and averaging the
squared errors provides an estimate of how accurately the model predicts
unobserved values within the observed range.

Subsequently, a MANOVA was conducted to explain differences in the RMSE and
GCV values between the simulation conditions and analysis methods. In order to
also find evidence in favor of which factors likely do not affect the RMSE and
GCV values, we conducted an exhaustive model search. Here the AIC and BIC
based model weights were used as a criterion to identify which model is closest
to the true data generating model. The AIC weights can directly
be interpreted as the conditional model probabilities
\parencite{wagenmakers_aic_2004}. Thus, if the true
data generating model is among the tested models, the AIC weights indicate the
probability that each tested model is the correct model given the data. This
makes it possible, to quantify evidence both in favor and against the presence
of any effect that could be included in the model. After establishing the most
likely MANOVA model, we probed the included effects further by fitting
univariate ANOVAs with the same predictors to the RMSE and the GCV values and
followed up with Tukey's Honest Significant Difference test for the
post-hoc comparisons.

\subsubsection{Uncertainty quantification}

To investigate the uncertainty estimates that each method provides around the
predicted process values, we recorded at each time point whether the true
generated process value was within the confidence intervals created by each
analysis method. Subsequently, by averaging over all time-points the average
confidence interval coverage proportion for each method and simulation
condition was obtained. Since, all obtained confidence or credible intervals
had a confidence level of 95\% the expected coverage proportion should ideally
be close to 95\%. Due to the Monte Carlo error of the simulation coverage
proportions between 93\% and 97\% were considered sufficiently close to the
expected 95\%. Conversely, coverage proportions larger than 97\% indicated
overestimated standard errors, whereas coverage proportions below 93\% were
interpreted to indicate either a poor approximation of the underlying process
or a underestimation of the standard errors.
